<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Wei Zhuo</title>
  
  <subtitle>始终在磕磕绊绊的摸索，至今依然是个无知的人</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-12-20T09:40:34.555Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Mario Z</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DiffPool:《Hierarchical Graph Representation Learning with Differentiable Pooling》阅读笔记</title>
    <link href="http://yoursite.com/2019/12/19/diffpool/"/>
    <id>http://yoursite.com/2019/12/19/diffpool/</id>
    <published>2019-12-19T11:32:36.000Z</published>
    <updated>2019-12-20T09:40:34.555Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址： <a href="https://arxiv.org/pdf/1806.08804.pdf" target="_blank" rel="noopener">DiffPool</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>传统的GNN算法在Node-level的任务如节点分类、链路预测上有着较好的效果。但是，现有的GNN方法由于其存在平面化的局限性，因此无法学习图的层级表示（意味着无法预测整个图的标签），顾无法实现图分类任务。举个栗子，一个Graph，可以分成600个subgraph，每个节点都存在于其中的某个subgraph（一个节点只存在于一个subgraph中），每个subgraph拥有一个标签，如何预测subgraph的标签是这篇文章主要想解决的问题。传统的GNN的图分类方法都是为Graph中的所有节点生成Embedding，然后将对这些Embedding做全局聚合（池化），如简单的把属于同一个subgraph的节点求和或者输入到MLP中生成一个标签向量来表示整个subgraph，但是这样可能忽略的图的层级结构信息。</p><p>本文提出了一种端到端的可微可微图池化模块<strong>DiffPool</strong>，原理如下图所示：  </p><p><img src="/2019/12/19/diffpool/1.png" alt=""></p><p>在深度GNN中的每层中为节点学习可微的软簇分配，将节点映射到簇中，这些簇作为新的节点作为下一层GNN的输入。上图的Original Network部分是一个Subgraph，传统的方法是直接求出这个Subgraph中每个节点的Embedding，然后相加或输入到一个神经网络中，得到一个预测向量，这种方法可以称为“全局池化”。<strong>DiffPool</strong>中，假设第$l$层的输入是$1000$个簇（如果是第一层输入就是1000个节点），我们先设置第$l+1$层需要输入的簇的个数（假设为$100$），也就是第$l$层输出的簇个数，然后在$l$层中通过一个分配矩阵将$1000$个簇做合并，合并成100个“节点”，然后将这100个节点输入到$l+1$层中，最后图中的节点数逐渐减少，最后，图中的节点只有一个，这个节点的embedding就是整个图的表示，然后将图输入到一个多层感知机MLP中，得到预测向量，在于真值的one-hot向量做cross-entropy，得到Loss。</p><h1 id="Model：DiffPool"><a href="#Model：DiffPool" class="headerlink" title="Model：DiffPool"></a>Model：DiffPool</h1><p>一个Graph表示为$\mathcal{G} = (A,F)$，其中$A \in \{0,1\}^{n \times n}$是Graph的邻接矩阵，$F \in \mathbb{R}^{n \times d}$表示节点特征矩阵，每个节点有$d$维的特征。给定一个带标签的子图集$\mathcal{D}=\left\{\left(G_{1}, y_{1}\right),\left(G_{2}, y_{2}\right), \ldots\right\}$， 其中 $y_{i} \in \mathcal{Y}$表示每个子图$G_i \in \mathcal{G}$的标签，任务目标是寻找映射$f: \mathcal{G} \rightarrow \mathcal{Y}$，将图映射到标签集。我们需要一个过程来将每个子图转化为一个有限维度的向量$\mathbb{R}^D$。</p><h2 id="Graph-Neural-Networks"><a href="#Graph-Neural-Networks" class="headerlink" title="Graph Neural Networks"></a>Graph Neural Networks</h2><p>一般，GNN可以表示成”Message Passing”框架：</p><script type="math/tex; mode=display">H^{(k)}=M\left(A, H^{(k-1)} ; \theta^{(k)}\right)</script><p>其中$H^{(k)} \in \mathbb{R}^{n \times d}$表示GNN迭代$k$次后的node embedding，$M$是一个Message扩散函数，由邻接矩阵$A$和一个可训练的参数$\theta^{(k)}$决定。$H^{(k-1)}$是由前一个message passing过程生成的node embedding。当$k = 1$时，第一个GNN的输入为$H^{(0)}$是原始的节点特征$H^{(0)} = F$。</p><p>GNN的一个主要目标是设计一个Message Passage函数$M$，GCN（kipf.2016）是一种流行的GNN，$M$的实现方式是将线性变换和ReLU非线性激活结合起来:</p><script type="math/tex; mode=display">H^{(k)}=M\left(A, H^{(k-1)} ; W^{(k)}\right)=\operatorname{ReLU}\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(k-1)} W^{(k-1)}\right)</script><p>其中，$\tilde{A} = A+I$是一个加上自环的邻接矩阵，$\tilde{D}=\sum_{j} \tilde{A}_{i j}$是$\tilde{A}$的度矩阵，$W^{(k)} \in \mathbb{R}^{d \times d}$是一个可训练的权重矩阵，$W$与节点个数以及每个节点的度无关，可以看做一个特征增强矩阵，用来规定GCN的输出维度。</p><p>一个完整的GNN模型会迭代$K$次来输出最终的node embedding$Z = H^{(K)} \in \mathbb{R}^{n \times d}$。对于GCN，GAT，GraphSage，$K$一般取2-6。文中为了简单表示，忽略了GNN的内部结构，用$Z=GNN(A,X)$来表示一个任意的执行$K$次的GNN模块。</p><h2 id="GNN和池化层的堆叠"><a href="#GNN和池化层的堆叠" class="headerlink" title="GNN和池化层的堆叠"></a>GNN和池化层的堆叠</h2><p>这篇工作的目标是定义一个一般的，端到端的可微策略，允许以层级的方式堆叠多个GNN模块。给定原始的邻接矩阵$A \in \mathbb{R}^{n \times n}$，$Z=GNN(A,X)$十一GNN模块的输出（假设这个GNN模块做了3次迭代）。我们需要定义一个策略来输出一个新的粗化图，这个粗化图包含$m$个节点，$m &lt; n$，它的邻接矩阵一个带权重的邻接矩阵$A’ \in \mathbb{R}^{m \times m}$，同时，输出node embedding $Z’ \in \mathbb{R}^{m \times d}$。这个粗化图（$m$个节点的图）作为下一层GNN的输入 （将$A’$和$Z’$输入下一个GNN层）。最后所有节点粗化为只有一个节点的图，这个节点的embedding就是这个subgraph的表示。因此，目标为：如何使用上一层GNN的输出结果，对节点做合并或池化，是的图中的节点减少，再将粗化的图输入到下一个GNN中。</p><h2 id="基于可学习分配的可微分池化"><a href="#基于可学习分配的可微分池化" class="headerlink" title="基于可学习分配的可微分池化"></a>基于可学习分配的可微分池化</h2><p><strong>DiffPool</strong>通过对一个GNN模块的输出学习一个聚类分配矩阵来解决这个问题。可微池化层根据$l-1$层的GNN模块（假设是一个3次迭代的GNN模块）产生的node embedding来对节点做合并，从而产生一个粗化图，这个粗化图作为$l$层GNN模块的输入，最终，整个subgraph被粗化为一个cluster，可以看做一个节点。</p><h3 id="用分配矩阵进行池化"><a href="#用分配矩阵进行池化" class="headerlink" title="用分配矩阵进行池化"></a>用分配矩阵进行池化</h3><p>$S^{(l)} \in \mathbb{R}^{n_{l} \times n_{l+1}}$表示第$l$层的聚类分配矩阵，$S^{(l)}$的每一行表示第l层的每个节点（cluster）,每一列表示$l+1$层的每个cluster（节点）。$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率，所以$S^{(l)}$是个概率矩阵。</p><p>假如已经有了第$l$层的节点分配矩阵$S^{(l)}$，将第$l$层的邻接矩阵表示为$A^{(l)}$，将第$l$层GNN模块的输出节点特征（node embedding）表示为$Z^{(l)}$，通过DiffPool层可以将第$l$层的图粗化为$\left(A^{(l+1)}, X^{(l+1)}\right)=\operatorname{DIFFPOOL}\left(A^{(l)}, Z^{(l)}\right)$，其中，$A^{(l+1)}$是$l+1$层图的邻接矩阵，是一个粗化后的图，$X^{(l+1)}$是下一层的输入特征（node/cluster embedding）：</p><script type="math/tex; mode=display">\begin{aligned}&X^{(l+1)}=S^{(l)^{T}} Z^{(l)} \in \mathbb{R}^{n_{l+1} \times d}\\&A^{(l+1)}=S^{(l)^{T}} A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1} \times n_{l+1}}\end{aligned}</script><p>上面第一个公式将第$l$层节点嵌入$Z^{(l)}$转化为下一层的输入特征$X^{(l+1)}$。第二个公式将第$l$层的邻接矩阵转化为$l+1$层的粗化图邻接矩阵$A^{(l+1)}$。$n_{l+1}$是$l+1$层节点（cluster）的数量。最后，将$A^{(l+1)}$和$X^{(l+1)}$作为下一层GNN的输入。这样图中的节点就由$n_l$个下降到$n_{l+1}$个。</p><h3 id="学习分配矩阵S"><a href="#学习分配矩阵S" class="headerlink" title="学习分配矩阵S"></a>学习分配矩阵S</h3><p>第$l$层的输入特征$X^{(l)}$，用一个GNN模块（代码中是一个3层的GCN）得到node embedding：</p><script type="math/tex; mode=display">Z^{(l)}=\mathrm{GNN}_{l, \text { embed }}\left(A^{(l)}, X^{(l)}\right)</script><p>用另外一个GNN模块（代码中是一个3层的GCN）在用一个softmax转化为概率矩阵来的到节点分配矩阵：</p><script type="math/tex; mode=display">S^{(l)}=\operatorname{softmax}\left(\mathrm{GNN}_{l, \mathrm{pool}}\left(A^{(l)}, X^{(l)}\right)\right)</script><p>$S^{(l)}$是一个$n_l \times n_{l+1}$的全链接矩阵，$S^{(l)}_{ij}$表示第$l$层的节点$i$属于第$l+1$层cluster $j$的概率。</p><p>$l=0$时，第一层GNN的输入是subgraph的原始邻接矩阵$A$和特征矩阵$F$，倒数第二层$l=L-1$时的分配矩阵$S^{(L-1)}$是一个全1向量，那么最后将所以节点归为一类，产生一个代表整个图的嵌入向量。</p><p>所以，把图节点的合并过程称为分层的图表示学习（Hierarchical Graph Representation Learning）。</p><h2 id="辅助链路预测和熵正则化"><a href="#辅助链路预测和熵正则化" class="headerlink" title="辅助链路预测和熵正则化"></a>辅助链路预测和熵正则化</h2><p>未完待续。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址： &lt;a href=&quot;https://arxiv.org/pdf/1806.08804.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DiffPool&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Intr
      
    
    </summary>
    
      <category term="Graph Mining" scheme="http://yoursite.com/categories/Graph-Mining/"/>
    
      <category term="Graph Neural Network" scheme="http://yoursite.com/categories/Graph-Mining/Graph-Neural-Network/"/>
    
    
      <category term="Graph Neural Network" scheme="http://yoursite.com/tags/Graph-Neural-Network/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
      <category term="Hierarchical Graph Embedding" scheme="http://yoursite.com/tags/Hierarchical-Graph-Embedding/"/>
    
  </entry>
  
  <entry>
    <title>谱聚类 Spectral Clustering 笔记</title>
    <link href="http://yoursite.com/2019/09/07/spectral-clustering/"/>
    <id>http://yoursite.com/2019/09/07/spectral-clustering/</id>
    <published>2019-09-07T01:11:09.000Z</published>
    <updated>2019-09-08T03:59:12.247Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。</p><p>本文主要参考了：[1] <a href="https://www.cnblogs.com/pinard/p/6221564.html#!comments" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6221564.html#!comments</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>谱聚类是图论中的算法，主要思想是空间中距离较远的两个点之间边的权重值较低， 距离较近的两个点之间边的权重值较高， 通过对所有数据点组成的图（Graph）进行切图， 使不同子图之间边的权重和尽可能低， 子图内边的权重值的和尽可能高，从而达到聚类的目的。</p><h1 id="基础1：-无向权重图"><a href="#基础1：-无向权重图" class="headerlink" title="基础1： 无向权重图"></a>基础1： 无向权重图</h1><p>对于边$(v_i,v_j)$, 它的权重$w_{ij} &gt; 0$。对于没有边的节点$v_i$和$v_j$,  他们之间的权重$w_{ij}=0$。图中的节点$v_i$, 它的度$d_i$定义为和它连接的所有边的权重之和，即：</p><script type="math/tex; mode=display">d_i = \sum_{j=1}^n w_{ij}</script><p>根据所有节点的度值，我们可以得到一个度矩阵$D$:</p><script type="math/tex; mode=display">D=\displaystyle \left(\begin{array}{ccc}{d_{1}} & {\ldots} & {\ldots} \\ {\ldots} & {d_{2}} & {\ldots} \\ {\vdots} & {\vdots} & {\ddots} \\ {\ldots} & {\ldots} & {d_{n}}\end{array}\right) ^{n\times n}</script><p>是一个$n \times n$的对角阵，对角元素是每个节点的度和。</p><p>定义图的邻接矩阵为$W \in \mathbb{R}^{n \times n}$， 每个元素$W_{ij}$表示节点对$(v_i,v_j)$之间的权重。 对于$V$中的一个子节点集$A \subset V$， 定义：</p><script type="math/tex; mode=display">|A|=A 中的节点个数 \\vol(A) = \sum_{i \in A} d_i \quad 表示A中所有节点的权重之和</script><h1 id="基础2：相似矩阵"><a href="#基础2：相似矩阵" class="headerlink" title="基础2：相似矩阵"></a>基础2：相似矩阵</h1><p>再提下谱聚类的基本思想： 距离较远的两个点之间权重值较低，距离较近的两个点之间权重值较高</p><p>但是在谱聚类中，我们只能获得数据点的定义，无法给出邻接矩阵，因为是离散的分布在空间中，无法得知其中的连接关系。</p><p>一般来说，通过样本点距离度量的相似矩阵$S$来获得邻接矩阵$W$.</p><p>构建邻接矩阵$W$有两种方法: $\epsilon$-邻近法， K邻近法和全连接法。</p><h2 id="epsilon-邻近法"><a href="#epsilon-邻近法" class="headerlink" title="$\epsilon$-邻近法"></a>$\epsilon$-邻近法</h2><p>$\epsilon$为距离的阈值，欧式距离$S_{ij}$表示两点$v_i$,$v_j$的坐标$x_i$,$x_j$之间的距离。 即 $S_{ij} =||x_i-x_j||^2_2$为相似矩阵$S \in \mathbb{R}^{n \times n}$的第$i$行第$j$个元素，则邻接矩阵$W$可以表示为：</p><script type="math/tex; mode=display">w_{i j}=\left\{\begin{array}{ll}{0} & {s_{i j}>\epsilon} \\ {\epsilon} & {s_{i j} \leq \epsilon}\end{array}\right.</script><p>意思是如果两点之间的距离大于$\epsilon$，那么他们之间的权重为0， 如果他们之间的距离小于$\epsilon$，他们之间的权重为$\epsilon$。 这种方法的弊端在于点之间的距离只有两种情况，不够精确，故很少采用。</p><h2 id="K邻近法"><a href="#K邻近法" class="headerlink" title="K邻近法"></a>K邻近法</h2><p>利用<strong>KNN</strong>算法遍历所有样本点，取每个样本点最近的$k$个点作为近邻，只有和样本点距离最近的$k$个点的$w_{ij} &gt;0$，但会出现一种情况， $v_i$的k个近邻中有$v_j$，但$v_j$的k个近邻中没有$v_i$，这样会造成邻接矩阵$W$的不对称， 因此提供两种解决办法</p><p>第一种： 只要$v_j$在$v_i$的K邻域中，那么不管$v_i$在不在$v_j$的K邻域中，都把$v_i$加入$v_j$的邻域中，即：</p><script type="math/tex; mode=display">w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} & {x_{i} \notin K N N\left(x_{j}\right) \text { and } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right)} & {x_{i} \in K N N\left(x_{j}\right) \text { or } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.</script><p>第二种，必须$v_i$在$v_j$的K邻域中，且$v_j$在$v_i$的K邻域中，那么才保留两者间的权重，否则都为0，即：</p><script type="math/tex; mode=display">w_{i j}=w_{j i}=\left\{\begin{array}{ll}{0} & {x_{i} \notin K N N\left(x_{j}\right) \text { or } x_{j} \notin K N N\left(x_{i}\right)} \\ {\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right)} & {x_{i} \in K N N\left(x_{j}\right) \text { and } x_{j} \in K N N\left(x_{i}\right)}\end{array}\right.</script><h2 id="全连接法"><a href="#全连接法" class="headerlink" title="全连接法"></a>全连接法</h2><p>设所有点之间的权重都大于0，可以选择不同的核函数来定义边的权重，常用的如多项式核函数，高斯核函数， Sigmod核函数。 最常用的为高斯核函数RBF，将两点之间的距离带入高斯核函数RBF中，即：</p><script type="math/tex; mode=display">w_{i j}=w_{ji}=s_{i j}=s_{ji}=\exp \left(-\frac{\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right)</script><p>其中，$sigma$为为函数的宽度参数 , 控制了函数的径向作用范围。 </p><h1 id="基础3：拉普拉斯矩阵"><a href="#基础3：拉普拉斯矩阵" class="headerlink" title="基础3：拉普拉斯矩阵"></a>基础3：拉普拉斯矩阵</h1><p>拉普拉斯矩阵定义为$L = D-W$ 如上文所示，$D$为度矩阵，$W$为邻接矩阵。</p><p>拉普拉斯矩阵具有如下性质：</p><ol><li><p>$L$是对称阵 （因为$D$和$W$都是对称阵）</p></li><li><p>$L$的所有特征值都是实数 （因为$L$是对称阵）</p></li><li><p>对于任意向量$f$， 有$f^TLf = \displaystyle \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n w_{ij} (f_i-f_j)^2$</p><p>推导：</p><script type="math/tex; mode=display">\begin{aligned}f^TLf &= f^TDf-f^TWf\\&= \sum^n_{i = 1}d_if_i^2 - \sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij}\\&= \frac{1}{2}\left(\sum^n_{i=1}d_if_i^2 -2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum^n_{i=1}d_if_i^2\right)\\&由于d_i = \sum_{j = 1}^nw_{ij}, 将d_i带入上式得\\f^TLf &= \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2\right)\\& = \frac{1}{2} \left(\sum_{i = 1}^n\sum_{j =1}^n w_{ij}f_i^2-2\sum_{i = 1}^n\sum_{j =1}^n f_if_j w_{ij} + \sum_{j = 1}^n\sum_{i =1}^n w_{ji}f_j^2\right)\\&= \frac{1}{2}\left(\sum_{i=1}^n\sum_{j = 1}^n w_{ij} (f_i-f_j)^2\right)\end{aligned}</script></li><li><p>拉普帕斯矩阵是半正定的，且$n$个实数特征值都$\geq$，即 $0=\lambda_1 \leq \lambda_2 \cdots \leq lambda_n$，且最小的特征值为0。</p><p>证明，因为$f^TLf \geq 0$ 所以$L$半正定。</p></li></ol><h1 id="基础4：无向图切图"><a href="#基础4：无向图切图" class="headerlink" title="基础4：无向图切图"></a>基础4：无向图切图</h1><p>对于无向图$G$，目的是将图$G = (v,E)$切成相互没有连接的k个子图，每个子图是一个节点集合：$A_1,A_2,\cdots, A_k$，满足$A_i \cap A_j = \phi$ 且$A_1 \cup A_2 \cup \cdots \cup A_k = V$，对于两个节点集合$A ,B \subset V$, $A \cap B = \phi$，定义$A$,$B$之间的切图权重为：</p><script type="math/tex; mode=display">W(A,B) = \sum_{v_i\in A, v_j \in B} w_{ij}  \quad 表示A中节点到B中节点的权重和</script><p>对于$k$个子图节点集合$A_1,A_2,\cdots, A_k$，定义切图$Cut$为：</p><script type="math/tex; mode=display">Cut(A_1,A_2, \cdots, A_k) = \frac{1}{2}\sum^k_{i = 1} W(A_i,\overline{A_i})</script><p>其中$\overline{A_i}$是$A_i$的补集，也就是除$A_i$外其他所有节点的集合，$Cut$计算的是$A_i$中每个节点到$\overline{A_i}$中每个节点的权重总和，如果最小化$Cut$，就相当于最小化每个子集中节点到自己外节点的权重，但是会存在一个问题，如下图所示：</p><p><img src="/2019/09/07/spectral-clustering/1.jpg" alt=""></p><p>如果按左边那条线分割，可以保证有边子图到左边子图的权重最小，但不是最佳分割。</p><h1 id="谱聚类：切图聚类"><a href="#谱聚类：切图聚类" class="headerlink" title="谱聚类：切图聚类"></a>谱聚类：切图聚类</h1><p>为了避免上述效果不佳的情况，提供两种切图方式，1.RatioCut, 2.Ncut.</p><h2 id="RatioCut-切图"><a href="#RatioCut-切图" class="headerlink" title="RatioCut 切图"></a>RatioCut 切图</h2><p>最小化$Cut(A_1,A_2, \cdots, A_k)$的同时，最大化每个子图中接待你的个数，即：</p><script type="math/tex; mode=display">RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A}_{i}\right)}{\left|A_{i}\right|}</script><p>目标是最小化$RatioCut\left(A_{1}, A_{2}, \ldots A_{k}\right)$。</p><p>为此，我们引入一个<strong>指示向量（indicator vector）</strong>$h_j \in {h_1,h_2,\cdots, h_k}$，其中$j = 1,2,\cdots,k$，对于其中任意一个向量$h_j$，它是一个$n$维向量，即：</p><script type="math/tex; mode=display">h_j = (h_{1j},h_{2j}, \cdots, h_{nj})^T \\h_{i j}=\left\{\begin{array}{ll}{0} & {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{\left|A_{j}\right|}}} & {v_{i} \in A_{j}}\end{array}\right.</script><p>$h_ij$表示节点$v_i$ 是否属于子图$A_j$, 如果属于，那么$h_{ij} = \frac{1}{\sqrt{\left|A_{j}\right|}}$，如果不属于，那么$h_{ij} = 0$。</p><p>那么对于$h_i^TLh_i$有：</p><script type="math/tex; mode=display">\begin{aligned}h_i^T L h_i &= \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\&= \frac{1}{2}\left(\sum_{m\in A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2+\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 + \\\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2 + \sum_{m\notin A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2\right)\\& 上式中的mn是图中任意选取的两个不同的节点， 针对子图A_i及其对应的指示向量h_i,\\&任意选取的节点对有四种情况。其中 根据h_{ij}的定义，第一项和第四项为0，所以\\&=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(h_{im}-h_{in})^2 +\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(h_{im}-h_{in})^2\right) \\&=\frac{1}{2} \left(\sum_{m\in A_i}\sum_{n \notin A_i}w_{mn}(\frac{1}{\sqrt{\left|A_{i}\right|}})^2 +\sum_{m\notin A_i}\sum_{n \in A_i}w_{mn}(-\frac{1}{\sqrt{\left|A_{i}\right|}})^2\right) \\&=\frac{1}{2}\left(\frac{1}{|A_i|}Cut(A_i,\overline{A_i}) + \frac{1}{|A_i|}Cut(A_i,\overline{A_i})\right)\\&=\frac{Cut(A_i,\overline{A_i})}{|A_i|} = RatioCut(A_i)\end{aligned}</script><p>上式$h_i^TLh_i$可以看做是子图$A_i$的RatioCut，那么：</p><script type="math/tex; mode=display">\begin{aligned}RatioCut(A_1,A_2,\cdots,A_k) &=\frac{1}{2} \sum_{i=1}^{k} \frac{W\left(A_{i}, \overline{A_i}\right)}{\left|A_{i}\right|}  = \sum^k_{i = 1} \frac{Cut(A_i,\overline{A_i})}{|A_i|}\\&=\sum^k_{i=1} h_i^TLh^i = \sum^k_{i =1} (H^TLH)_{ii} = tr(H^TLH)\end{aligned}</script><p>每个$h_i^TLh^j$的值对应于矩阵$H^TLH$在位置$ij$处的值：</p><script type="math/tex; mode=display">H=(h_1,h_2,\cdots,h_k) \in \mathbb{R}^{n\times k} \\h_i^TLh_j = (H^TLH)_{ij} \to h^T_iLh_i = (H^TLH)_{ii}\\由于h_i\cdot h_j = 0 \quad h_i \cdot h_i = 1, 所以H^TH = I是一个单位矩阵</script><p>所以切图优化函数为：</p><script type="math/tex; mode=display">\underbrace{\arg \min }_{H} RatioCut\left(A_1,A_2,\cdots A_k\right) = \underbrace{\arg \min }_{H} \operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. } \quad H^{T} H=I</script><p>$H$中的每个指示向量$h$是$n$维的，每个向量中的元素有两种取值，分别是0和$\frac{1}{\sqrt{\left|A_{j}\right|}}$， 所以每个$h$有$2^n$种可能性，所以整个$H$有$k2^n$中，因此上述目标函数是个NP-hard问题。</p><p>注意到$tr(H^TLH)$中每个优化子目标$h_i^TLh_i$，其中，$h$是单位正交基，基每个元素的平方和等于1，$L$为对称矩阵，<strong>此时$h^T_iLh_i$的最大值为$L$的最大特征值， $h^T_iLh_i$的最小值是$L$的最小特征值</strong>。在谱聚类中，我们的目标是找到目标函数的最小特征值，从而使目标函数最小，得到对应的特征向量。</p><p>对于$h^T_iLh_i$，目标是找到$L$最小的特征值，这个值就是$h^T_iLh_i$的最小值，对于$tr(H^TLH) = \sum^k_{i=1} h^T_iLh_i$，目标是找到$k$个最小的特征值，从而使$tr(H^TLH)$最小。</p><p>通过找到$L$最小的$k$个特征值，可以对应得到$k$个特征向量，这$k$个特征向量可以组成一个$n \times k$的矩阵，这个矩阵就是我们需要的指示向量矩阵$H$，里面包含了每个节点所属子图的信息。一般来说 我们需要对$H$按行做标准化：</p><script type="math/tex; mode=display">h_{ij}^* = \frac{h_{ij}}{\sqrt{\sum_{t=1}^kh^2_{it}}}</script><p>注意到，$H$的每行是一个$k$维行向量，即$H_i = (H_{i1},H_{i2},\cdots, H_{ik})$， 表示节点$i$属于每个子图的指标值， 我们可以把$H_i$当做节点$v_i$的表示向量， 由于归一化后的$H$还不能明确指示各个样本的归属， 我们还需要对$H$代表的所有节点做一次传统聚类，如K-Means。</p><h2 id="NCut切图"><a href="#NCut切图" class="headerlink" title="NCut切图"></a>NCut切图</h2><p>把$RatioCut$的分母从$|A_i|$换成$vol(A_i) = \sum_{j \in A_i}d_j$， 为$A_i$中所有节点的权重之和，一般来说$NCut$效果好于$RatioCut$:</p><script type="math/tex; mode=display">NCut(A_1,A_2,\cdots,A_k) = \frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\overline{A_i})}{vol(A_i)} = \sum^k_{i = 1}\frac{Cut(A_i)}{vol(A_i)}</script><p>$NCut$指示向量$h$做了改进，$RatioCut$切图在指示向量中使用$\frac{1}{\sqrt{|A_i|}}$表示某节点归属于子图$A_i$，而$NCut$切图使用子图权重$\frac{1}{\sqrt{vol{A_i}}}$来表示某节点归属子图$A_i$如下：</p><script type="math/tex; mode=display">h_{i j}=\left\{\begin{array}{ll}{0} & {v_{i} \notin A_{j}} \\ {\frac{1}{\sqrt{v o l\left(A_{j}\right)}}} & {v_{i} \in A_{j}}\end{array}\right.</script><p>上式表示如果节点$v_i$在子图$A_j$中，那么指示向量$h_j$的第$i$个元素为$\frac{1}{\sqrt{vol{A_j}}}$。</p><p>那么对于$h_i^TLh_i$有：</p><script type="math/tex; mode=display">h^T_iLh_i = \frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2 = \frac{Cut(A_i)}{vol(A_i)} =NCut(A_i)</script><p>目标函数：</p><script type="math/tex; mode=display">NCut(A_i,A_2,\cdots,A_k) = \sum^k_{i = 1} NCut(A_i) = \sum^k_{i=1}h^T_iLh_i =\sum^k_{i=1}(H^TLH)_{ii} = tr(H^TLH)</script><p>此时，$h_i \cdot h_j = 0$，$h_i\cdot h_i = \frac{|A_i|}{vol(A_i)} \neq 1$， 所以$H^TH \neq I$。</p><p>但是， 由于：</p><script type="math/tex; mode=display">h^T_iDh_i = \sum^n_{j = 1} h_{ij}^2d_j\\d_j为节点v_j的权重和，h_{ij}的值表示节点j是否在子图A_i中，\\如果在子图A_i中，那么h_{ij}^2 = \frac{1}{vol(A_i)}，否则为0。\\h^T_iDh_i = \frac{1}{vol(A_i)} \sum_{v_j \in A_i} d_j = \frac{1}{vol(A_i)} \cdot vol(A_i) = 1</script><p>最终目标函数为：</p><script type="math/tex; mode=display">\underbrace{\arg \min } _{H}\operatorname{tr}\left(H^{T} L H\right) \quad \text { s.t. }\quad H^{T} D H=I</script><p>由于$H$中的指示向量$h$不是标准正交基，所以RatioCut中加粗的定理不能直接使用，所以，令$H = D^{-\frac{1}{2}}F$, $D^{-\frac{1}{2}}$表示对$D$对角线上元素开方后求逆，那么：</p><script type="math/tex; mode=display">H^TLH = F^TD^{-\frac{1}{2}}LD^{-\frac{1}{2}}F\\H^TDH = F^TD^{-\frac{1}{2}}DD^{-\frac{1}{2}}F = F^TF=I</script><p>所以目标函数转化为：</p><script type="math/tex; mode=display">\underbrace{\arg \min }_{F} \operatorname{tr}\left(F^{T} D^{-1 / 2} L D^{-1 / 2} F\right) \quad \text { s.t. } \quad F^{T} F=I</script><p>同$RatioCut$，$D^{-1 / 2} L D^{-1 / 2}$是对称矩阵，$F$中的每个向量为标准正交基，只需要求出$D^{-1 / 2} L D^{-1 / 2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，最后得到特征矩阵$F$，然后对$F$的行向量做一次传统聚类，如K-means.</p><p>一般来说， $D^{-1 / 2} L D^{-1 / 2}$相当于对拉普拉斯矩阵$L$做了一次标准化，即$\frac{L_{i j}}{\sqrt{d_{i} * d_{j}}}$.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在看GNN/GCN的时候遇到谱聚类，学了下，做了些笔记。&lt;/p&gt;
&lt;p&gt;本文主要参考了：[1] &lt;a href=&quot;https://www.cnblogs.com/pinard/p/6221564.html#!comments&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
      <category term="Machine Leaning" scheme="http://yoursite.com/categories/Machine-Leaning/"/>
    
    
      <category term="Clustering" scheme="http://yoursite.com/tags/Clustering/"/>
    
      <category term="Spectral Clustering" scheme="http://yoursite.com/tags/Spectral-Clustering/"/>
    
      <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>《metapath2vec:Scalable Representation Learning for Heterogeneous Networks》阅读笔记</title>
    <link href="http://yoursite.com/2019/06/29/metapath/"/>
    <id>http://yoursite.com/2019/06/29/metapath/</id>
    <published>2019-06-29T08:29:18.000Z</published>
    <updated>2019-06-29T10:56:30.496Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址： <a href="https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf" target="_blank" rel="noopener">metapath2vec</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>真实网络中通常包含多种类型的节点和关系，这些节点和关系共同组成了异质信息网络（Heterogeneous Information Network），传统的NE方法如DeepWalk, Line , node2vec更多关注与同质网络从而忽略了网络的异质性。针对这个问题，这篇文章提出了metapath2vec基于meta-path的随机游走来构建节点的异质邻域，采用异质Skip-gram模型来学习节点的representation。 另外，在负采样时也考虑异质性，从而进一步提出了metapath2vec++。如下：</p><p><img src="/2019/06/29/metapath/1.png" alt=""></p><h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><p><strong>Definition 1:</strong> 异质网络： $G = (V,E,T)$，其中每个节点和边分别对应一个映射 $\phi(v) : V \rightarrow T_{V}$ 和$\varphi(e) : E \rightarrow T_{E}$。 其中$T_V$和$T_E$分别表示节点的类型集合以及关系的类型集合。且$\left|T_{V}\right|+\left|T_{E}\right|&gt;2$。</p><p><strong>Definition 2:</strong> 异质网络表示学习 为网络中的节点学习一个$d$维的表示 $\mathrm{X} \in \mathbb{R}^{|V| \times d}$，$d \ll|V|$。节点的向量表示可以捕获节点间结构和语义关系。</p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="metapath2vec"><a href="#metapath2vec" class="headerlink" title="metapath2vec"></a>metapath2vec</h2><p>Skip-Gram模型是给定一个节点，预测向下文节点的概率。metapath2vec的目的是考虑多种类型节点和多种类型关系的情况下，最大化网络概率。为了将异质信息融入skip-gram模型中，首先提出异质skip-gram。</p><h3 id="Heterogeneous-Skip-Gram"><a href="#Heterogeneous-Skip-Gram" class="headerlink" title="Heterogeneous Skip-Gram"></a>Heterogeneous Skip-Gram</h3><p>对于节点类型$|T_V| &gt; 1$的网络$G$， 给定一个节点$v$，需要最大化属于类型$t \in T_V$的异质上下文节点的概率： </p><script type="math/tex; mode=display">\arg \max _{\theta} \sum_{v \in V} \sum_{t \in T_{V}} \sum_{c_{t} \in N_{t}(v)} \log p\left(c_{t} | v ; \theta\right)</script><p>其中，$N_t(v)$表示$v$属于第$t$个类型的邻居节点。 $p\left(c_{t} | v ; \theta\right)$ 表示给定节点$v$，它的$t$类型邻域概率： $p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u \in V} e^{X_{u} \cdot X_{v}}}$，是一个softmax函数，$X_v$是节点$v$的表示。由于softmax函数的分母每一次都要遍历所有节点，所以采用负采样改进$\log p(\cdot)$ 如下： </p><script type="math/tex; mode=display">\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u^{m} \sim P(u)}\left[\log \sigma\left(-X_{u^{m}} \cdot X_{v}\right)\right]</script><p>其中 $\sigma(x)=\frac{1}{1+e^{-x}}$。</p><h3 id="Meta-Path-Based-Random-Walks"><a href="#Meta-Path-Based-Random-Walks" class="headerlink" title="Meta-Path-Based Random Walks"></a>Meta-Path-Based Random Walks</h3><p>在第$i$步时，转移概率$p(v^{i+1}|v^i)$表示为忽略节点类型情况下$v^i$的邻居分布。但是，PathSim提出，异质信息网络中的随机游走偏向于高度可见的节点，即具有主导数量路径的节点，所以 本文设计了基于元路径的随机游走来生成path，从而能够捕获不同类型节点间的结构联系和语义关系，提出了促进异构网络结构转换为metapath2vec的skip-gram。</p><p>一个meta-path模式$\mathcal{P}: V_{1} \stackrel{R_{1}}{\longrightarrow} V_{2} \stackrel{R_{2}}{\longrightarrow} \dots V_{t} \stackrel{R_{t}}{\longrightarrow} V_{t+1} \cdots \stackrel{R_{l-1}}{\longrightarrow} V_{l}$， 其中 $R=R_{1} \circ R_{2} \circ \cdots \circ R_{l-1}$ 节点类型$V_{1}$到$V_{l}$之间的组合关系。那么节点间的跳转概率定义为：</p><script type="math/tex; mode=display">p\left(v^{i+1} | v_{t}^{i}, \mathcal{P}\right)=\left\{\begin{array}{cc}{\frac{1}{\left|N_{t+1}\left(v_{t}^{i}\right)\right|}} & {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right)=t+1} \\ {0} & {\left(v^{i+1}, v_{t}^{i}\right) \in E, \phi\left(v^{i+1}\right) \neq t+1} \\ {0} & {\left(v^{i+1}, v_{t}^{i}\right) \notin E}\end{array}\right.</script><p>其中$v^i_t \in V_t$，$N_{t+1}\left(v_{t}^{i}\right)$表示属于$t$类型的节点$v$的属于$t+1$类型的邻居。如果下一个节点$v^{i+1}$和$v^i_t$之间有边，并且$v^{i+1}$是$t+1$类型的节点 那么转移概率服从平均分布。其中，$v^{i+1}$服从meta-path所定义的下移节点类型。如图（a）中，原路径为$OAPVPAO$，那么节点$a_4$的下一个节点必然要是$P$类。 由于meta-path的对称性，所以：</p><script type="math/tex; mode=display">p\left(v^{i+1} | v_{t}^{i}\right)=p\left(v^{i+1} | v_{1}^{i}\right), \text { if } t=l</script><h2 id="metapath2vec-1"><a href="#metapath2vec-1" class="headerlink" title="metapath2vec++"></a>metapath2vec++</h2><p>由于softmax做归一化时没有考虑节点类型，分母是对所有节点求和，所以为了融合节点类型，给出<strong>Heterogeneous negative sampling</strong>: </p><script type="math/tex; mode=display">p\left(c_{t} | v ; \theta\right)=\frac{e^{X_{c_{t}} \cdot X_{v}}}{\sum_{u_{t} \in V_{t}} e^{X_{u_{t}} \cdot X_{v}}}</script><p><strong>如图（c）所示，metapath2vec++对每种类型节点指定不同的一组多项式分布</strong>，相当于在输出层根据节点类型，把异质网络分解成不同的同质网络，同样采用负采用的方法简化计算：</p><script type="math/tex; mode=display">O(\mathrm{X})=\log \sigma\left(X_{c_{t}} \cdot X_{v}\right)+\sum_{m=1}^{M} \mathbb{E}_{u_{t}^{m} \sim P_{t}\left(u_{t}\right)}\left[\log \sigma\left(-X_{u_{t}^{m}} \cdot X_{v}\right)\right]</script><p>算法如下：</p><p><img src="/2019/06/29/metapath/2.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址： &lt;a href=&quot;https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;metapath2vec&lt;
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="Meta-path" scheme="http://yoursite.com/tags/Meta-path/"/>
    
      <category term="Heterogeneous Information Networks" scheme="http://yoursite.com/tags/Heterogeneous-Information-Networks/"/>
    
  </entry>
  
  <entry>
    <title>M-NMF:《Community Preserving Network Embedding》阅读笔记</title>
    <link href="http://yoursite.com/2019/05/29/MNMF/"/>
    <id>http://yoursite.com/2019/05/29/MNMF/</id>
    <published>2019-05-29T02:46:44.000Z</published>
    <updated>2019-05-29T06:22:52.514Z</updated>
    
    <content type="html"><![CDATA[<p>  论文地址：<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14589" target="_blank" rel="noopener">M-NMF</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Network Embedding的基本需求是保存网络机构和固有属性。而先前的方法主要保存了网络的围观结构（microscopic structure）如一阶相似度和二阶相似度（LINE，SDNE等）， 忽略了网络结构的介观社区结构（mesoscopic community structure）。针对这个问题，本文提出了一种模块化非负矩阵分解模型M-NMF，该模型将网络的社区结构融入network embedding中。利用节点表示和社区结构之间的一致关系（consensus relationship）， 联合优化基于非负矩阵分解的表示学习模型和基于模块化的社区发现模型， 这样就可以在节点表示中同时保存了<strong>微观网络结构</strong>和<strong>介观社区结构</strong>。</p><p>具体来说，对于微观结构（节点对相似性），本文使用矩阵分解来融合节点的一节相似性和二阶相似性；对于介观社区结构，通过模块度约束项来检测社区。因此，上面的两项可以通过节点的表示和基于辅助社区表示矩阵社区结构之间的一致关系来联合优化。同时，本文给出了乘法更新策略，保证了再推导参数的时候的正确性和收敛性。</p><h1 id="M-NMF-Model"><a href="#M-NMF-Model" class="headerlink" title="M-NMF Model"></a>M-NMF Model</h1><p>对于一个无向图$G=(V,E)$, 图中有$n$个节点和$e$条边，用一个0,1二值邻接矩阵表示为$\mathbf{A}=[A_{i,j}] \in \mathbb{R}^{n \times n}$。 $\mathbf{U} \in \mathbb{R}^{n \times m}$ 为节点的表示矩阵，其中$m \leq n$，$m$是节点的嵌入维度。</p><h2 id="建模社区结构"><a href="#建模社区结构" class="headerlink" title="建模社区结构"></a>建模社区结构</h2><p>本文采用基于模块最大化的社区发现方法 Modularity， 给定一个邻接矩阵表示的网络$\mathbf{A}$，$\mathbf{A}$包含两个社区，根据<a href="http://engr.case.edu/ray_soumya/mlrg/2006%20Modularity%20and%20community%20structure%20in%20networks.pdf" target="_blank" rel="noopener">Newman 2006b</a>，模块度可以定义如下：</p><script type="math/tex; mode=display">Q=\frac{1}{4 e} \sum_{i j}\left(A_{i j}-\frac{k_{i} k_{j}}{2 e}\right) h_{i} h_{j}</script><p>其中，$k_i$表示节点$i$的度，如果$i$在第一个社区中，那么$h_i=1$,否则$h_i = -1$。</p><p>$k_ik_j$表示将所有边一分为二 参考<a href="https://blog.csdn.net/wangyibo0201/article/details/52048248" target="_blank" rel="noopener">模块度Q</a>，那么节点$i$,$j$之间可能产生的边数。$\frac{k_{i} k_{j}}{2 e}$如果所有边随机放置，节点$i$,$j$之间的期望边数。定义一个模块度矩阵$\mathbf{B} \in \mathbb{R}^{n \times n}$，其中$B_{i,j}=A_{i,j}-\frac{k_{i} k_{j}}{2 e}$，那么$Q=\frac{1}{4 e} \mathbf{h}^{T} \mathbf{B h}$，其中$\mathbf{h}=[h_i] \in \mathbb{R}^n$，表示社区成员指标器。</p><p>如果将$Q$拓展到$k &gt; 2$个社区，那么：</p><script type="math/tex; mode=display">Q=\operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B H}\right), \quad \text { s.t. } \quad \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{H}\right)=n</script><p>其中$tr()$表示矩阵的迹（主对角线元素和），$\mathbf{H}$是社区成员指标器，$\mathbf{H} \in \mathbb{R}^{n \times k}$，每行表示一个节点所属社区的one-hot编码。</p><h2 id="建模微观结构"><a href="#建模微观结构" class="headerlink" title="建模微观结构"></a>建模微观结构</h2><h3 id="一阶相似度"><a href="#一阶相似度" class="headerlink" title="一阶相似度"></a>一阶相似度</h3><script type="math/tex; mode=display">S^{(1)} = \mathbf{A}</script><h3 id="二阶相似度"><a href="#二阶相似度" class="headerlink" title="二阶相似度"></a>二阶相似度</h3><p>表示为$S^{(2)}$，表示节点的邻域相似度， 用邻接向量的余弦相似度表示：</p><script type="math/tex; mode=display">S_{i j}^{(2)}=\frac{\mathcal{N}_{i} \mathcal{N}_{j}}{\left\|\mathcal{N}_{i}\right\|\left\|\mathcal{N}_{j}\right\|}</script><p>结合网络的一阶结构的一阶二阶相似度，最终的网络相似度矩阵可以表示为：</p><script type="math/tex; mode=display">\mathbf{S}^{(1)}+\eta \mathbf{S}^{(2)}</script><p>然后，文中引入了一个偏置矩阵$\mathbf{M} \in \mathbb{R}^{n \times m}$ 和一个非负表示矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$。所以微观结构的目标函数就是节点相似度和节点表示之间的误差：</p><script type="math/tex; mode=display">\min \left\|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right\|_{F}^{2}</script><p>因为$\mathbf{S} \in \mathbb{R}^{n \times n}$， 矩阵$\mathbf{U} \in \mathbb{R}^{n \times m}$，矩阵$\mathbf{M}$的作用是把$\mathbf{U}$转成$n \times n$，这样就可以计算损失函数了。</p><h2 id="统一的NE模型"><a href="#统一的NE模型" class="headerlink" title="统一的NE模型"></a>统一的NE模型</h2><p>引入一个非负辅助矩阵$\mathbf{C} \in \mathbb{R}^{k \times m}$, 即为社区表示矩阵，每一行$C_r$表示第$r$个社区的$m$维表示向量。 如果一个节点的表示向量和一个社区的表示向量接近，那么这个节点就很可能在这个社区中。我们把节点$i$和社区$r$之间的从属关系定义为：</p><script type="math/tex; mode=display">\mathbf{U}_{i} \mathbf{C}_{r}</script><p>如果两个向量正交，则$\mathbf{U}_{i} \mathbf{C}_{r} = 0$ 那么节点$i$不可能存在于社区$r$中。所以需要使$\mathbf{U}\mathbf{C}^T$更加近似社区指示器$\mathbf{H}$,所以定义如下目标函数：</p><script type="math/tex; mode=display">\min _{\mathbf{M}, \mathbf{U}, \mathbf{H}, \mathbf{C}}\left\|\mathbf{S}-\mathbf{M} \mathbf{U}^{T}\right\|_{F}^{2}+\alpha\left\|\mathbf{H}-\mathbf{U} \mathbf{C}^{T}\right\|_{F}^{2}-\beta \operatorname{tr}\left(\mathbf{H}^{T} \mathbf{B} \mathbf{H}\right) \\s.t., \mathrm{M} \geqslant 0, \mathrm{U} \geqslant 0, \mathrm{H} \geqslant 0, \mathrm{C} \geqslant 0, \operatorname{tr}\left(\mathrm{H}^{T} \mathrm{H}\right)=n</script><p>其中$\alpha$和$\beta$是正参数，最后一项是要最大化模块度。</p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>由于目标函数非凸，顾分三个子问题优化。 具体太多不想看了。。。 瞟了一眼，应该看不懂，拜拜~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;  论文地址：&lt;a href=&quot;https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14589&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;M-NMF&lt;/a&gt;&lt;/p&gt;
&lt;h1 id
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="MF" scheme="http://yoursite.com/tags/MF/"/>
    
      <category term="Community Detection" scheme="http://yoursite.com/tags/Community-Detection/"/>
    
  </entry>
  
  <entry>
    <title>《Representation Learning for Information Diffusion through Social Networks:an Embedded Cascade Model》阅读笔记</title>
    <link href="http://yoursite.com/2019/05/12/embedding-ic/"/>
    <id>http://yoursite.com/2019/05/12/embedding-ic/</id>
    <published>2019-05-12T14:09:38.000Z</published>
    <updated>2019-05-13T08:43:58.632Z</updated>
    
    <content type="html"><![CDATA[<p> 文章链接：<a href="https://dl.acm.org/citation.cfm?id=2835817" target="_blank" rel="noopener">Embedding_IC</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文基于Network Embedding优化了传统的独立级联模型（IC），用以学习一个更具鲁棒性的扩散概率。</p><p>对于网络和社交数据，分析用户交互来为扩散现象建模具有以下困难：</p><ul><li>用户通常是异质的，伴随着不同的行为或兴趣。并且他们通过各种通道同时交互。</li><li>用户之间的关系很难检测和表征。 例如，在信息传播过程中，不经常交互的用户之间的“弱关系”很重要，而它们很难被捕获。</li><li>不同应用中的级联长度变化很大，难以学习和预测。</li></ul><p>本文的目的是通过从观察中学习来开发扩散模型。为此，本文专注于独立级联模型（IC），该模型定义了网络中的扩散迭代过程。在这种情况下，建模扩散的问题归结为<strong>学习表征用户间隐含的相互影响关系的概率分布</strong>，以便发现网络的主要通信通道。我们在此考虑的模型执行以下假设：</p><ul><li>影响传播是二元的（被感染或不被感染），</li><li>扩散网络未知，</li><li>影响关系不依赖于传播的内容，</li><li>用户之间的感染概率不会随时间变化（一旦分配了两个节点间的感染概率，那么该概率固定）。</li></ul><p>本文基于NE，将扩散空间建模为一个连续的潜在空间，用户间的相对位置用于定义内容传输的可能性。用户靠的越近，传播概率越高。如下图所示：</p><p><img src="/2019/05/12/embedding-ic/1.png" alt=""></p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h2><p>传播事件集$\mathcal{D}=\{D_1,D_2,…,D_n\}$ 可以视为一个graph中的$n$个级联（cascade）。其中，一个传播事件$D$是一个带有时间戳的用户集。值得注意的是，级联中的用户并不知道是谁传染它，仅知道它何时被传染。</p><p>给定一个社交网络，有$N$个用户：$\mathcal{U}=\{u_1,u_2,…,u_N\}$，一个传播级联$D$可以定义为user和它的timestamp的集合：$D = \{(u,t^D(u)) | u \in \mathcal{U} \wedge t^D(u)&lt; \infty\}$， 其中，$t^D: \mathcal{U} \to \mathbb{R}^+$ 表示用户被传染的时间戳， $\infty$表示未被传染的用户。每个用户的时间戳$t^D$表示该用户相对于源用户（source user）被传染的时间，$t^D(source \quad user) = 0$。接下来，用$D(t)$表示在时间戳$t$之前被传染的用户：$D(t) = \{u \in \mathcal{U} | t^D(u)&lt;t\}$。对称地，用$\bar{D}(t)$表示在时间戳$t$之前没有被感染的用户。 $D(\infty)$表示最终所有被感染的用户，$\bar{D}(infty)$表示最终所有没被感染的用户。</p><h2 id="Diffusion-Model"><a href="#Diffusion-Model" class="headerlink" title="Diffusion Model"></a>Diffusion Model</h2><p>本文将级联embed到连续空间中，用以捕获扩散关系的规律性。本文只考虑传播的顺序，而不是确切的感染时间戳。</p><p>在Embedding_IC中，在一个传播事件(级联)$D$中，一个用户是否被感染的概率取决于先前所有已经被感染的节点。给定一个已经被影响的用户集$I \in \mathcal{U}$, $P(v|I)$表示给定活跃用户集$I$时，节点$v$被传染的概率，其中$v \in \mathcal{U} \backslash I$:</p><script type="math/tex; mode=display">P(v|I) = 1-\prod_{u \in I}(1-P_{u,v})</script><p>上式中，$\prod_{u \in I}(1-P_{u,v})$表示$I$中所有节点都不影响$v$的概率。那么$P(v|I)$就可以表示$v$被感染的概率。</p><p>接下来就需要给出$P_{u,v}$的定义了，即$v$被$u$传染的概率。$z_u \in \mathbb{R}^d$是传染源用户$u$的表示向量，$\omega_{v} \in \mathbb{R}^d$是传染目标用户$v$的表示向量。那么$P_{u,v}$可以定义如下：</p><script type="math/tex; mode=display">P_{u,v} = f(z_u,\omega_{v})</script><p>其中，$f: \mathbb{R}^d \times \mathbb{R}^d \to [0,1]$，是一个映射函数，把两个表示向量映射到概率空间：</p><script type="math/tex; mode=display">f\left(z_{u}, \omega_{v}\right)=\frac{1}{1+\exp \left(z_{u}^{(0)}+\omega_{v}^{(0)}+\sum_{i=1}^{d-1}\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)^{2}\right)}</script><p>其中，$z_{u}^{(i)}$和$\omega_{v}^{(i)}$分别表示$z_u$和$\omega_v$的第$i$个分量。表示随距离增加而递减的传输概率，即$\left(z_{u}^{(i)}-\omega_{v}^{(i)}\right)$越大$f$越小。上式使用了sigmoid函数:$\frac{1}{1+e^{-x}}$返回一个$[0,1]$的概率。</p><p>值得注意的是，偏置项$z_{u}^{(0)}$和$\omega_{v}^{(0)}$的作用是反映$u$传入$v$的一般趋势，这样做的目的是避免不同的$u$和$v$产生相同的概率。</p><h2 id="Learning-Algorithm"><a href="#Learning-Algorithm" class="headerlink" title="Learning Algorithm"></a>Learning Algorithm</h2><p>考虑所有节点对的传播概率$\mathcal{P}=\{P_{u,v} | (u,v) \in \mathcal{U}^2\}$ (涉及所有节点对)。那么对于特定级联$D$的概率为：</p><script type="math/tex; mode=display">P(D)=\prod_{v \in D(\infty)} P_{v}^{D} \prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)</script><p>上式中，$\prod_{v \in D(\infty)} P_{v}^{D}$表示$D$中所有被影响的用户存在的概率，$\prod_{v \in \overline{D}(\infty)}\left(1-P_{v}^{D}\right)$表示$D$中所有未被影响的用户存在的概率。所以$P(D)$就是级联$D$存在的概率。同时，可以用对数似然来表示训练级联集$\mathcal{D}$:</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{P} ; \mathcal{D})=\sum_{D \in \mathcal{D}}\left(\sum_{v \in D(\infty)} \log \left(P_{v}^{D}\right)+\sum_{v \in \overline{D}(\infty)} \log \left(1-P_{v}^{D}\right)\right)</script><p>上式就是模型的目标函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 文章链接：&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=2835817&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Embedding_IC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="Diffusion" scheme="http://yoursite.com/categories/Diffusion/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Diffusion/paper/"/>
    
    
      <category term="Social Network Analysis" scheme="http://yoursite.com/tags/Social-Network-Analysis/"/>
    
      <category term="Diffusion" scheme="http://yoursite.com/tags/Diffusion/"/>
    
      <category term="Network Embedding" scheme="http://yoursite.com/tags/Network-Embedding/"/>
    
  </entry>
  
  <entry>
    <title>Influence Maximization Conclusion</title>
    <link href="http://yoursite.com/2019/05/06/IM-conclusion/"/>
    <id>http://yoursite.com/2019/05/06/IM-conclusion/</id>
    <published>2019-05-06T12:16:17.000Z</published>
    <updated>2019-05-07T03:09:20.732Z</updated>
    
    <content type="html"><![CDATA[<h4 id="影响力传播模型"><a href="#影响力传播模型" class="headerlink" title="影响力传播模型"></a>影响力传播模型</h4><p>社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">LC IT模型（独立级联模型和线性阈值模型）</span><br><span class="line"></span><br><span class="line">WC（权重级联模型）</span><br><span class="line"></span><br><span class="line">HD（热传播模型）</span><br><span class="line"></span><br><span class="line">SIR（传染病模型）</span><br><span class="line"></span><br><span class="line">MIA模型（路径相关）</span><br><span class="line"></span><br><span class="line">投票模型</span><br><span class="line"></span><br><span class="line">巴斯模型</span><br></pre></td></tr></table></figure><h4 id="影响力最大化算法"><a href="#影响力最大化算法" class="headerlink" title="影响力最大化算法"></a>影响力最大化算法</h4><p>目前有的几个影响力最大化的算法</p><blockquote><ul><li><p>基于目标函数的子模性质的算法（一般是贪心算法和贪心算法的改进）</p><ul><li><p>A note on maximizing a submodular set function subject to a knapsack constraint<br>这个文章主要是把问题扩展了，认为选择每个节点是有不同的代价的，而不同于传统的问题，传统的问题中，每个节点的选择大家都是一样的，因此，在总代价一定的情况下，那么问题就是选择固定个数的节点即可，但是如果每个节点的选择代价不一样，在总代价一定的情况下，选择的节点的个数就不一样</p></li><li><p>Cost-effective outbreak detection in networks （CELF算法）</p></li><li><p>Celf ++ : optimizing the greedy algorithm for influence maximization in social networks（CELF++算法）<br>这个文章就是大名鼎鼎的CELF算法，利用子模性质和边际效益递减对贪心算法进行优化，同理还有CELF++算法也是大致相同的思路</p></li></ul></li><li><p>基于中心性的启发式算法</p><ul><li><p>Efficient influence maximization in social networks W.Chen （DegreeDiscount算法）<br>这个算法也是启发式算法，主要在于，每当选择一个节点后，认为这个节点不会再被影响，因此，它的邻居节点地度-1</p><ul><li><p>A potential-based node selection strategy for influence max- imization in a social network （TM算法）<br>这个算法也是一种启发式算法，它是以节点地潜在地影响力作为启发的因素去选择初始的节点，同时设置了一个参数c，分为两个阶段，第一个阶段启发式的选择潜在影响力最大的若干个节点，第二阶段用贪心算法对这些第一阶段选择的种子节点进行筛选，当参数c为0，也就是没有第一阶段，那么算法就变成了传统的贪心算法。算法的缺点也很明显，首先参数c的选择依赖于经验判断，其次是启发式，准确率没有保证，再次也需要使用贪心算法，因此时间复杂度很高</p></li><li><p>A new centrality measure for influence maxi- mization in social networks<br>考虑到了传播模型中的传播度，利用这个进行启发式选择传播度最高的节点，从而得到更加精确的结果。这个传播度包括了两个方面，第一是节点自己影响他的邻居，第二个是他的邻居影响其他节点，结合影响概率一起构建的模型。优点在于启发式的选择的同时考虑了传播模型。</p></li></ul></li></ul></li><li><p>基于影响路径的算法</p><ul><li><p>Tractable models for information diffusion in social networks（SP1M算法）<br>基于影响路径的算法考虑某个节点只会尽可能地影响从这个节点开始地最短或者次最短路径上地节点，因此，可以递归地计算influenc spread而不用像贪心算法那样使用蒙特卡洛模拟，从而导致大量地计算时间，因此提高了算法地效率</p><ul><li><p>Scalable influence maximization for prevalent viral marketing in large-scale social networks （MIA算法）<br>借鉴了基于路径地影响力最大化算法的思路，提出一种利用局部图结构的树状近似算法来近似influence spread从而也是避免了蒙特卡罗模拟。此算法中，每条路径具有一个传播概率，定义为在这条路径上的每条边的传播概率的乘积。只有具有最大传播概率的路径才能够作为影响力路径来扩散影响力。同时给每一个节点计算树状度，定义为从节点出发的各条路径中，路径传播概率大于阈值 $\theta$的路径上的所有点的集合</p></li><li><p>Scalable and parallelizable processing of influence maximization for large-scale social networks （IPA算法）</p></li></ul></li></ul><p>该算法不同于chen等人提出的算法，认为每条路径是相互独立的，chen等人只选择了具有最大的propagation 概率的那条路径，但是本论文则选择所有大于阈值 $\theta$的路径，并行的计算他们的influence spread。基于路径的算法也具有缺点，比如没有理论上的准确度保证，同时，针对于特别复杂的图，空间复杂度非常大</p></li><li><p>基于社区的算法</p><ul><li><p>Oasnet: an optimal allocation approach to in- fluence maximization in modular social networks（OASNET算法）</p><p>这个算法假设社交网络划分社区后的每个社区是相互独立的，社区之间不会存在相互的影响力传播，利用CNM算法进行社区发现。种子节点的选择则分为两个阶段，第一个节点在每个社区内部利用贪心算法选择k个节点，第二个阶段则使用动态规划的方法在$C \times k$个节点中选择最终的k个节点</p></li><li><p>Identifying influential nodes in complex networks with community structure</p><p>这个算法基于利用社区结构发现社交网络中的最具有影响力的几个节点的研究。首先根据加权图构造概率转移矩阵，然后使用$K-Mediods$聚类方法找到最具有影响力的若干个节点。</p></li><li><p>Cim: community-based influence maximization in social networks（CIM算法）</p><p>chen等人基于HD（热传播）模型提出的基于社区结构的影响力最大化算法。算法分为好三个阶段，首先是社区发现，作者给出了一种$H_{Clustering}$算法用于社区发现，然后是候选节点迭代，作者根据节点的拓扑结构和它的社区特征进行选择，最后是种子节点的选择，同时考虑了诸多因素，个人认为是一个比较合理的影响力最大化算法。</p></li><li><p>Conformity-aware influence maximization in online social networks （CINEMA算法）</p><p>基于节点的一致性来设计的算法。传播模型中的概率定义为让第一个节点的影响力指标乘以第二个节点的一致性指标作为传播概率。</p><p>当然，基于社区发现的算法也有自己的缺点，首先是在社区内部进行初步的节点的选择，也需要进行蒙特卡洛模拟，因此时间复杂度也会比较大，其次，社区发现的思路，是用节点在社区内的influence spread去模拟它在whole network上的influence spread，近似效果依赖于网络结构，如果社区之间的连接边都比较少，那么近似结果是非常接近的，但是如果社区之间的连接边比较多，及即是hub节点比较多，那么近似效果可想而知</p></li></ul></li></ul></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.基于的贪心算法KK（kempe等学者提出的算法）</span><br><span class="line">2.基于贪心算法的改进算法，利用启发式规则改进的NewGreedyIC，MixedGreedyIC，NewGreedyWC算法</span><br><span class="line">相关论文：Efficient influence maximization in social networks 2009</span><br><span class="line">3.基于贪心算法的改进算法，利用子模性质改进的CELF算法，改进的CELF++算法</span><br><span class="line">相关论文：Cost-effective outbreak detection in networks 2007</span><br><span class="line">CELF++：optimizing the greedy algorithm for influence maximization in social networks 2011</span><br><span class="line">4.启发式算法：随机算法，度中心算法，MaxDegree算法，Degree Discount算法</span><br><span class="line">相关论文：Efficient influence maximization in social networks</span><br><span class="line">5.基于社区划分的算法：OASNET算法，CGA算法等（后面加上现阶段阅读的论文）</span><br><span class="line">相关论文：Community-based greedy algorithm for mining top-k influential nodes in mobile social networks 2010</span><br><span class="line">6.MIA算法</span><br><span class="line">混合式算法</span><br></pre></td></tr></table></figure><h4 id="社区划分的算法"><a href="#社区划分的算法" class="headerlink" title="社区划分的算法"></a>社区划分的算法</h4><blockquote><ul><li>基于模块优化的算法</li><li>光谱聚类的算法</li><li>层次分级的算法</li><li>基于标签传播的算法<ul><li>LPA算法（目前的最快的社区划分算法，几乎是线性时间复杂度）</li></ul></li></ul></blockquote><h4 id="论文思考的几个点"><a href="#论文思考的几个点" class="headerlink" title="论文思考的几个点"></a>论文思考的几个点</h4><p>基于社区发现的影响力最大化算法的分析，论文研究的目的：</p><ol><li>算法的效率保证，时间复杂度尽可能低。</li><li>算法的性能保证，尽可能接近最优解。（利用到子模拟性质？）</li></ol><p>毕业论文的大致的框架，总体是基于社区划分的思路，具体需要做的工作如下：</p><blockquote><ul><li><p><strong>传播模型的选择</strong>，如何改进传播模型使得切合实际的传播过程？IC，LT改进？结合PageRank算法或者思想？或者考虑改进HDM传播模型？引入时间空间的因素使得模型更加充分？</p><p>   （1）传播模型的改进，传播模型中，针对于某个节点的从邻居获得的影响力，不应该简单的直接叠加，而是考虑每个邻居并不是等同对待的，应当区分不同的权重，针对节点之间的互动频率，互动频率高的节点，应该具有更加高的信任度，同时，也可能存在负面的影响力，即反而让节点更不可能选择新产品，这点应该在改进的模型中有所反馈。</p><p>   （2）至于这个信任程度如何计算出来反应在传播模型中，则可以考虑，根据邻居之间的互动信息，每个节点的活跃程度，邻居节点本身是否是具有很高的度的节点，邻居节点和本节点的观点是正相关还是反相关，从而决定邻居对本节点的信任度。</p><p>   （3）应该考虑影响力的时效性，是否可以考虑结合HDM和LTM模型一起，加上信任度参数这个观点，一起构建一个新的传播模型。</p></li><li><p><strong>社区发现算法的选择</strong>，社区发现的选择是非常重点的，社区发现本质上是社交网络节点的聚类，应该涉及比较有效率的聚类算法或者选择其他的距离算法？</p><p>   社区发现聚类算法，一般都是先设置每个节点作为单独的一个社区，然后进行合并，在进行社区聚类发现的时候，不应当单独仅仅考虑边，仅仅利用边的关系，比如CGA算法就利用到了传播模型，结合传播模型进行标签传播，然后获得相应的划分的社区。同时，可以加以改进的地方，比如，社交网络的社区发现不应当仅仅考虑到拓扑结构，还有考虑节点之间的互动交流的信息，互动程度越频繁，那么两个节点在一个社区内部的概率就越大，因此要考虑这个改进点。</p><p>   同时，借鉴了CGA算法的思想，一个节点的社区内部的影响力和整个社交网络的影响力如何区别？如何用社区内部的影响力去近似？或者考虑hub节点，社区之间的这些连接节点也有着非常重要的作用。</p></li><li><p><strong>社区发现是否可以处理重叠社区的情况</strong>，重叠社区会导致影响力的重复传播，如何减少这种情况的出现，如何设计算法实现重叠社区的处理？</p></li><li><p><strong>各个社区的重要性也是不同的，应该有选择的摒弃一些社区，先给出一个社区选择的模型，比如说利用PageRank先计算出哪些社区比较重要</strong>，有的社区人数多，有的社区人数少，但是处在中心位置，并且一些非重要的社区，往往会关注这些重要社区的传递出来的信息。考虑种子节点选择的时候，应当把社区这些因素考虑进去。我们可以忽略那些不重要的小社区，重要的社区给与比较大的加权值，同时注意影响力避免重叠传播。</p></li><li><p><strong>社区发现之后，如何分配每个社区的种子节点数目？</strong>，直接按照比例分配？亦或是选择一种度量社区重要程度的模型？</p><p>   CGA算法，使用了动态规划进行贪心选择，在各个社区内部选择相应的种子节点。但是时间复杂度仍然是非常大的，是否可以考虑先在社区内部基于启发式规则，或者PageRank，计算重要的节点，然后全局进行贪心的选择？</p></li><li><p><strong>基于社区发现的算法，实际上是利用节点在社区内的传播来近似它在整个网络上传播的效果</strong>，因此这种近似肯定存在误差，如何减少这种误差的产生？而且这种误差还是和网络中的社区结构有关系的。</p></li><li><p><strong>注意充分利用社区结构的特点，划分社区之后，把社区也视为一个点，作为整体去考虑</strong></p></li><li><p><strong>社区内的候选节点选择</strong>，候选节点应该按照什么标准进行选择，是按照启发式的度选择？还是设计另外的模型？结合PageRank模型？</p></li><li><p><strong>种子节点的获取策略</strong>，如何在这些候选节点上选择出最终的种子节点？直接按照贪心策略暴力选择还是参考CELF算法进行选择？或者是涉及其他的方法？</p></li></ul></blockquote><h4 id="思路总结"><a href="#思路总结" class="headerlink" title="思路总结"></a>思路总结</h4><blockquote><p>基于社区发现的影响力最大化算法框架：</p><ul><li>社区划分。</li><li>充分考虑社区作为一个整体性，来体现社区的一个作用，可以利用PageRank模型，来代表社区的重要程度，这个是社区的一个属性，利用这个模型，选择一些重要的社区，同时摒弃一些小的，没那么重要的社区。而且利用PageRank进行迭代，应该比传统的算法会快一些。</li><li>社区内部种子节点的选择，考虑的是社区内部的种子节点在社区内部的影响力传播。如何选择社区内部的种子节点。？？？？？这一点目前还需要多家考虑。</li><li>社区出现重叠，如何考虑？</li><li>种子节点在社区内的影响力只是对种子节点在全局的影响力的近似，那么需要一种方式来弥补这种误差。显然，种子节点的影响力如果想要传播到另外的社区，那么是通过社区之间的边界节点进行传播的，同时和社区本身的重要性有关，那么这部分的误差通过边界节点来弥补。</li><li>最终会得到若干的候选节点，这些节点使用贪心算法进行选择出最终的种子节点，考虑贪心算法的时间复杂度，那么可以考虑使用CELF思路或者是其他的CGA这类的动态规划思路去求解，不过这还是基于蒙特卡罗模拟，时间复杂度仍然相对比较大，对算法进行加速。</li><li>影响力传播模型，基于线性阈值模型进行改变，加上信任度参数，因为每个影响力的叠加不是平权的，和用户之间的互动，观点信息，兴趣爱好是否一直存在着相关的关系，因此在影响力传播模型中加入这个考虑因素。</li></ul></blockquote><h4 id="算法的实验部分，考虑一些经典的BaseLine-Algorithm"><a href="#算法的实验部分，考虑一些经典的BaseLine-Algorithm" class="headerlink" title="算法的实验部分，考虑一些经典的BaseLine Algorithm"></a>算法的实验部分，考虑一些经典的BaseLine Algorithm</h4><blockquote><ul><li>传统贪心爬山算法-KK算法</li><li>CELF算法</li><li>CGA算法</li><li>CIM算法</li><li>启发式算法（度启发式，中心性启发式）DegreeDiscount算法</li><li><p>本论文的算法</p></li><li><p>算法中参数的选择的影响</p></li><li>控制变量对比实验</li></ul></blockquote><h4 id="相关工作总结"><a href="#相关工作总结" class="headerlink" title="相关工作总结"></a>相关工作总结</h4><blockquote><ol><li><p>Richardson和Domingos在2002年的论文，首次把这个问题作为一个研究方向提出。</p></li><li><p>首先应该数说到的式Kempe的2003年的论文，主要提出了</p></li></ol><p>a）LT IC 模型，并说明了这个问题的NP完全性</p><p>b）给出了贪心算法</p><p>c）说明了影响力递增的边界递减性质，利用子模性质说明了算法的性能保证</p></blockquote><h4 id="近期论文阅读总结"><a href="#近期论文阅读总结" class="headerlink" title="近期论文阅读总结"></a>近期论文阅读总结</h4><ol><li>传播模型的改进，基于PageRank的改进，传统PageRank在考虑某个节点的PR值是均匀分配给链出的节点的（链出的概率为出度的倒数）（即权重级联模型），但是实际上，PR高的节点具有更高的影响力，因此考虑链出的概率不用度，而用PR值的占比，从而更加切合实际的情况。</li><li>还有的改进算法，改进了PageRank计算模型，把节点自身的属性，节点之间互动的属性，加入到了PageRank模型计算中，使得PageRank能够适用于社交网络中节点重要性的计算。</li><li><p>我们可以考虑把以上的两者结合起来给出一种新的信息传播模型（给出概率计算的方法）。</p></li><li><p>数据集选择</p><blockquote><p>参考宫秀云那篇文章</p></blockquote></li><li><p>总结：基于PageRank思想的影响力计算，都是在PageRank的基础上进行模型的改进，加入其他的影响因子，给出不同的权重，从而更加符合实际的应用场景。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;影响力传播模型&quot;&gt;&lt;a href=&quot;#影响力传播模型&quot; class=&quot;headerlink&quot; title=&quot;影响力传播模型&quot;&gt;&lt;/a&gt;影响力传播模型&lt;/h4&gt;&lt;p&gt;社交网络上影响力最大化问题需要借助相应的影响力传播模型，影响力传播模型的选择&lt;/p&gt;
&lt;figure
      
    
    </summary>
    
      <category term="IM" scheme="http://yoursite.com/categories/IM/"/>
    
    
      <category term="Social Network Analysis" scheme="http://yoursite.com/tags/Social-Network-Analysis/"/>
    
      <category term="Influence Maximization" scheme="http://yoursite.com/tags/Influence-Maximization/"/>
    
  </entry>
  
  <entry>
    <title>长期更新-好论文解读收藏</title>
    <link href="http://yoursite.com/2019/04/28/paper-unscramble/"/>
    <id>http://yoursite.com/2019/04/28/paper-unscramble/</id>
    <published>2019-04-28T08:36:50.000Z</published>
    <updated>2020-04-29T03:23:44.782Z</updated>
    
    <content type="html"><![CDATA[<p> <strong>这篇笔记用于收藏别人的论文解读</strong></p><h1 id="Social-Diffusion"><a href="#Social-Diffusion" class="headerlink" title="Social Diffusion"></a>Social Diffusion</h1><div class="table-container"><table><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Unscramble</th></tr></thead><tbody><tr><td style="text-align:center">Social Influence Locality for Modeling Retweeting Behavior</td><td style="text-align:center"><a href="http://www.datasnail.cn/papers/2018/03/01/group_influence.html" target="_blank" rel="noopener">解读</a>  <a href="https://www.aminer.cn/influencelocality" target="_blank" rel="noopener">代码</a></td></tr><tr><td style="text-align:center">Role-Aware Conformity Influence Modeling and Analysis in Social Network</td><td style="text-align:center"><a href="http://www.datasnail.cn/papers/2018/03/19/social_influence_role.html" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">DeepInf:Social Influence Prediction with Deep Learning</td><td style="text-align:center"><a href="https://redtongue.github.io/2018/11/10/DeepInf-Social-Influence-Prediction-with-Deep-Learning/" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Reverse Influence Sampling in Python</td><td style="text-align:center"><a href="https://hautahi.com/im_ris" target="_blank" rel="noopener">解读及代码</a></td></tr><tr><td style="text-align:center">Cost-effective Outbreak Detection in Networks（CELF）</td><td style="text-align:center"><a href="https://hautahi.com/im_greedycelf" target="_blank" rel="noopener">解读及代码</a> <a href="https://www.cnblogs.com/aaronhoo/p/6548760.html" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">DeepCas: an End-to-end Predictor of Information Cascades</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/24430190" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Inf2vec: Latent Representation Model for Social Influence Embedding</td><td style="text-align:center"><a href="https://iii.run/archives/286.html" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">POI2Vec: Geographical Latent Representation for Predicting Future Visitors</td><td style="text-align:center"><a href="https://blog.csdn.net/u014568072/article/details/78633812" target="_blank" rel="noopener">解读</a></td></tr></tbody></table></div><h1 id="Network-Embedding-and-GNN"><a href="#Network-Embedding-and-GNN" class="headerlink" title="Network Embedding and GNN"></a>Network Embedding and GNN</h1><div class="table-container"><table><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Unscramble</th></tr></thead><tbody><tr><td style="text-align:center">Heterogeneous Graph Attention Network</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s/hzwp5oGspdtDyNBmq8sMsw" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Attributed Social Network Embedding</td><td style="text-align:center"><a href="https://blog.csdn.net/sparkexpert/article/details/78066298" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Self-Translation Network Embedding</td><td style="text-align:center"><a href="https://www.cnblogs.com/chaoran/p/9872918.html" target="_blank" rel="noopener">解读1</a> <a href="https://zhuanlan.zhihu.com/p/45538566" target="_blank" rel="noopener">解读2</a></td></tr><tr><td style="text-align:center">Self-Paced Network Embedding</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/55104326" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">CANE: Context-Aware Network Embedding for Relation Modeling</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/42750132" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s/7dsrvHp6KIvlE-VXiUH1Rw" target="_blank" rel="noopener">解读1</a> <a href="http://rootlu.com/blog/2017/11/16/HIN2Vec.html/" target="_blank" rel="noopener">解读2</a></td></tr><tr><td style="text-align:center">Embedding Temporal Network via Neighborhood Formation</td><td style="text-align:center"><a href="http://rootlu.com/blog/2018/09/15/HTNE.html/" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Network Representation Learning with Rich Text Information</td><td style="text-align:center"><a href="http://rootlu.com/blog/2017/08/07/TextDeepWalk.html/" target="_blank" rel="noopener">解读1</a> <a href="https://blog.csdn.net/zhangbaoanhadoop/article/details/82313123" target="_blank" rel="noopener">解读2</a></td></tr><tr><td style="text-align:center">Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks</td><td style="text-align:center"><a href="http://rootlu.com/blog/2017/11/04/Esim.html/" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/34437681" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Context-Aware Network Embedding for Relation Modeling</td><td style="text-align:center"><a href="https://www.cnblogs.com/chaoran/p/9720558.html#top" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">GAN在网络特征学习中的应用</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s/V1XX8WWstR00m_OimMrDDw" target="_blank" rel="noopener">this</a></td></tr><tr><td style="text-align:center">一文读懂「Attention is All You Need」 附代码实现</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247486960&amp;idx=1&amp;sn=1b4b9d7ec7a9f40fa8a9df6b6f53bbfb&amp;chksm=96e9d270a19e5b668875392da1d1aaa28ffd0af17d44f7ee81c2754c78cc35edf2e35be2c6a1&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">node2vec: Scalable Feature Learning for Networks</td><td style="text-align:center"><a href="https://blog.csdn.net/nemoyy/article/details/81842536" target="_blank" rel="noopener">解读1</a> <a href="http://rootlu.com/blog/2017/09/28/Node2vec.html/" target="_blank" rel="noopener">解读2</a> <a href="https://zhuanlan.zhihu.com/p/38432396" target="_blank" rel="noopener">解读3</a></td></tr><tr><td style="text-align:center">PTE:Predictive Text Embedding through Large-scale</td><td style="text-align:center"><a href="http://rootlu.com/blog/2017/09/12/PTE.html/" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">struc2vec: Learning Node Representations from Structural Identity</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/42559966" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">AspEm: Embedding Learning by Aspects in HINs</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/46067193" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Max-Margin DeepWalk: Discriminative Learning of Network Representation</td><td style="text-align:center"><a href="https://www.jianshu.com/p/be27d1be7a79" target="_blank" rel="noopener">解读1</a> <a href="https://zhuanlan.zhihu.com/p/22660025" target="_blank" rel="noopener">解读2</a></td></tr><tr><td style="text-align:center">Structural Deep Network Embedding</td><td style="text-align:center"><a href="http://www.rootlu.com/blog/2017/11/14/SDNE.html/" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Learning Structural Node Embeddings via Diffusion Wavelets</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/50212921" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">HARP: Hierarchical Representation Learning for Networks</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/32525850" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Dynamic Network Embedding by Modeling Triadic Closure Process</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/34544862" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">RaRE: Social Rank Regulated Large-scale Network Embedding</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/44755895" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">TransNet: Translation-Based Network Representation Learning for Social Relation Extraction</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/35700043" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/47622003" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Graph Convolutional Network</td><td style="text-align:center"><a href="https://www.zhihu.com/question/54504471" target="_blank" rel="noopener">解答</a></td></tr><tr><td style="text-align:center">GraRep: Learning Graph Representations with Global Structural Information</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/46446600" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Deep Dynamic Network Embedding for Link Prediction</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/43224084" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Representation Learning for Attributed Multiplex Heterogeneous Network</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s?__biz=MzU1MTkwNzIyOQ==&amp;mid=2247488986&amp;idx=1&amp;sn=518dee71731df7013ab0c610a0bb2b98&amp;chksm=fb8b6f28ccfce63eec8a61c592b4d56f9a530a052e6339006fab617bedfdcd6e64996e0d1cf8&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;key=5472509901065d0595e5a5c5f212483c3d2f3efeeaef7669474af772e5e8c26230f46e4993f58e1ce0750728887bcacdbbfaeee9ed04016d855c9d372554841a90a974c8f49359d359e196635f9f0a9c&amp;ascene=1&amp;uin=MjU1NDA0MzIyOA%3D%3D&amp;devicetype=Windows+10&amp;version=62060825&amp;lang=zh_CN&amp;pass_ticket=Lc9QD7DhlGI4GwJMmLL%2Fqb%2FjUlOsqjZa063bAf%2FwTgv87Ysjj0jQmYfggMpsJh6o" target="_blank" rel="noopener">GATNE</a></td></tr><tr><td style="text-align:center">Inductive Representation Learning on Large Graphs</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/62750137" target="_blank" rel="noopener">GraphSAGE</a></td></tr><tr><td style="text-align:center">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/44197242" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/62471519" target="_blank" rel="noopener">PinSAGE</a></td></tr><tr><td style="text-align:center">Semi-Supervised Classification with Graph Convolutional Networks</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/65276194" target="_blank" rel="noopener">Semi-GCN</a></td></tr><tr><td style="text-align:center">Joint Type Inference on Entities and Relations via Graph Convolutional Networks</td><td style="text-align:center"><a href="https://blog.csdn.net/a609640147/article/details/93212524" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">图卷积神经网络（GCN）</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/71200936" target="_blank" rel="noopener">GCN</a></td></tr><tr><td style="text-align:center">Adaptive SamplingTowards Fast Graph Representation Learning</td><td style="text-align:center"><a href="https://jhy1993.github.io/Jhy1993.github.io/2019/05/28/18NIPS_Adaptive-Sampling-Towards-Fast-Graph-Representation-Learning/" target="_blank" rel="noopener">解读</a></td></tr><tr><td style="text-align:center">ProNE: Fast and Scalable Network Representation Learning</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/77063856" target="_blank" rel="noopener">ProNE</a></td></tr><tr><td style="text-align:center">ROTATE:Knowledge graph embedding by relational rotate in complex space</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s/R-KLHsYLbb8WHRjk6gwrhw" target="_blank" rel="noopener">Rotate</a></td></tr><tr><td style="text-align:center">Signed Graph Convolutional Network</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDE5MTQxNg==&amp;mid=2649692736&amp;idx=1&amp;sn=1d4ca82f51e467cb4318f92348b3c11f&amp;chksm=f276df97c501568136ac61346d1fec16acc3d9a02dcc6185aec4d7064255e4d2ed383190eac4&amp;token=1159444933&amp;lang=zh_CN#rd" target="_blank" rel="noopener">SGCN</a></td></tr><tr><td style="text-align:center">GAT: Graph Attention Network</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDE5MTQxNg==&amp;mid=2649694052&amp;idx=1&amp;sn=6132e91e1e6619bd9432025d4a52481c&amp;chksm=f276dab3c50153a585e71c3ac4f5742c0009d31eb4c40f423b9d7719675037993ca1e30286dd&amp;token=1221786093&amp;lang=zh_CN#rd" target="_blank" rel="noopener">GAT</a> <a href="https://zhuanlan.zhihu.com/p/34232818" target="_blank" rel="noopener">GAT2</a></td></tr><tr><td style="text-align:center">Large-Scale Learnable Graph Convolutional Networks</td><td style="text-align:center"><a href="https://www.jianshu.com/p/ada8730913ce" target="_blank" rel="noopener">LGCN</a> <a href="https://zhuanlan.zhihu.com/p/48834333" target="_blank" rel="noopener">LGCN2</a> <a href="https://zhuanlan.zhihu.com/p/53205116" target="_blank" rel="noopener">LGCN3</a> <a href="https://davidham3.github.io/blog/2018/09/17/large-scale-learnable-graph-convolutional-networks/" target="_blank" rel="noopener">LGCN4</a> <a href="https://blog.csdn.net/yyl424525/article/details/100057863" target="_blank" rel="noopener">LGCN5</a> <a href="https://www.jianshu.com/p/a099dfd46c61" target="_blank" rel="noopener">代码分析</a></td></tr><tr><td style="text-align:center">Hierarchical Graph Representation Learning with Differentiable Pooling</td><td style="text-align:center"><a href="https://blog.csdn.net/wanchaochaochao/article/details/81091295" target="_blank" rel="noopener">DiffPool</a> <a href="https://blog.csdn.net/weixin_40740160/article/details/83022408" target="_blank" rel="noopener">DiffPool</a></td></tr></tbody></table></div><h1 id="ML-amp-DL"><a href="#ML-amp-DL" class="headerlink" title="ML &amp; DL"></a>ML &amp; DL</h1><div class="table-container"><table><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Unscramble</th></tr></thead><tbody><tr><td style="text-align:center">常见散度与距离(KL散度，JS散度，Wasserstein距离，互信息MI)</td><td style="text-align:center"><a href="https://jhy1993.github.io/Jhy1993.github.io/2019/03/25/%E5%B8%B8%E8%A7%81%E6%95%A3%E5%BA%A6%E8%B7%9D%E7%A6%BB/" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">【简化数据】奇异值分解(SVD)</td><td style="text-align:center"><a href="https://blog.csdn.net/u012162613/article/details/42214205" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">AUC的计算与近似</td><td style="text-align:center"><a href="https://jhy1993.github.io/Jhy1993.github.io/2018/12/25/AUC%E7%9A%84%E8%AE%A1%E7%AE%97%E4%B8%8E%E8%BF%91%E4%BC%BC/" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">PCA</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/44371812" target="_blank" rel="noopener">Here</a> <a href="https://www.matongxue.com/madocs/1025.html" target="_blank" rel="noopener">Here</a> <a href="https://blog.csdn.net/hustqb/article/details/78394058" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">AdaBoost</td><td style="text-align:center"><a href="https://blog.csdn.net/px_528/article/details/72963977" target="_blank" rel="noopener">Here</a> <a href="https://blog.csdn.net/guyuealian/article/details/70995333" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">Batch Normalization</td><td style="text-align:center"><a href="https://zhuanlan.zhihu.com/p/38176412" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">矩阵的正定及半正定</td><td style="text-align:center"><a href="https://www.zhihu.com/question/22098422" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">精确率、召回率、F1 值、ROC、AUC</td><td style="text-align:center"><a href="https://www.zhihu.com/question/30643044" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">Hierarchical Softmax</td><td style="text-align:center"><a href="qiancy.com/2016/08/17/word2vec-hierarchical-softmax/">Here</a></td></tr><tr><td style="text-align:center">傅立叶变换</td><td style="text-align:center"><a href="https://mp.weixin.qq.com/s/kPbmne-PDd7nMsRrs0FigA" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">卷积神经网络（CNN）之一维卷积、二维卷积、三维卷积详解</td><td style="text-align:center"><a href="https://www.cnblogs.com/szxspark/p/8445406.html" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">变分自编码器（VAE）</td><td style="text-align:center"><a href="https://fenghz.github.io/Variational-AutoEncoder/" target="_blank" rel="noopener">Here</a></td></tr></tbody></table></div><h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><div class="table-container"><table><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Unscramble</th></tr></thead><tbody><tr><td style="text-align:center">tf.slice()</td><td style="text-align:center"><a href="https://www.jianshu.com/p/71e6ef6c121b" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">tf.tile()</td><td style="text-align:center"><a href="https://blog.csdn.net/qq1483661204/article/details/79681180" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">TensorFlow中global_step的简单分析</td><td style="text-align:center"><a href="https://blog.csdn.net/leviopku/article/details/78508951" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">tensorflow之tf.nn.l2_normalize与l2_loss的计算</td><td style="text-align:center"><a href="https://blog.csdn.net/qq_30638831/article/details/83473822" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">tf.nn.top_k()</td><td style="text-align:center"><a href="https://blog.csdn.net/m0_37393514/article/details/82380109" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">python中matplotlib的颜色及线条控制</td><td style="text-align:center"><a href="https://www.cnblogs.com/darkknightzh/p/6117528.html" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">python set(集合) &amp; 与 and  与 or之间的区别</td><td style="text-align:center"><a href="https://blog.csdn.net/qq_37195276/article/details/79467917" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">numpy实现top_k</td><td style="text-align:center"><a href="https://blog.csdn.net/SoftPoeter/article/details/86629329" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">np reshape(-1,1)</td><td style="text-align:center"><a href="https://blog.csdn.net/wld914674505/article/details/80460042" target="_blank" rel="noopener">Here</a></td></tr><tr><td style="text-align:center">傅里叶变换</td><td style="text-align:center"><a href="https://en.wikipedia.org/wiki/File:Fourier_series_and_transform.gif" target="_blank" rel="noopener">Here</a> <a href="https://zhuanlan.zhihu.com/p/19763358" target="_blank" rel="noopener">Here</a> Here</td></tr></tbody></table></div><h1 id="技术博客"><a href="#技术博客" class="headerlink" title="技术博客"></a>技术博客</h1><p><a href="https://davidham3.github.io/blog" target="_blank" rel="noopener">https://davidham3.github.io/blog</a></p><p><a href="https://fenghz.github.io/index.html" target="_blank" rel="noopener">https://fenghz.github.io/index.html</a></p><p><a href="https://archwalker.github.io/" target="_blank" rel="noopener">https://archwalker.github.io/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; &lt;strong&gt;这篇笔记用于收藏别人的论文解读&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;Social-Diffusion&quot;&gt;&lt;a href=&quot;#Social-Diffusion&quot; class=&quot;headerlink&quot; title=&quot;Social Diffusion&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="notebook" scheme="http://yoursite.com/categories/notebook/"/>
    
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>GraphWave:《Learning Structural Node Embeddings via DiffusionWavelets》代码笔记</title>
    <link href="http://yoursite.com/2019/04/24/graphWave/"/>
    <id>http://yoursite.com/2019/04/24/graphWave/</id>
    <published>2019-04-24T08:59:13.000Z</published>
    <updated>2019-04-24T09:58:19.966Z</updated>
    
    <content type="html"><![CDATA[<p> 论文地址：<a href="https://dl.acm.org/citation.cfm?id=3220025" target="_blank" rel="noopener">GraphWave</a></p><p>参考：<a href="https://zhuanlan.zhihu.com/p/50212921" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50212921</a></p><p><a href="https://github.com/benedekrozemberczki/GraphWaveMachine" target="_blank" rel="noopener">https://github.com/benedekrozemberczki/GraphWaveMachine</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>说实话这篇论文后面的数学证明部分完全看不懂~，最后结合<a href="https://github.com/benedekrozemberczki" target="_blank" rel="noopener">benedekrozemberczki</a>的代码强行给自己解释了一波。。。  </p><p>这篇文章的出发点是考虑到在网络图中不同部分的节点也可能具有相同的Embedding。如下图所示：</p><p><img src="/2019/04/24/graphWave/1.jpg" alt=""></p><p>节点$a$和$b$在网络中的距离较远，但是由于他们的邻域是相似的，也就是说他们具有结构相似性，所以$a$和$b$应该是相似节点。上图中的两个小柱状图现实节点$a$和节点$b$的小波系数分布很相似。现实网络中如社交网络中的管理员，细胞分子网络中的酶都符合这样的分布。</p><p>本文创新：</p><ul><li>完全非监督，不需要任何先验知识。</li><li>完整的数学证明，以前的方法都是启发式的，这篇论文作者使用大量篇幅证明使用GraphWave，结构等价/相似的节点具有近乎相同/相似的嵌入。</li></ul><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>class WaveletMachine：</p><p> 注： </p><p>PyGSP: Python中的图形信号处理</p><p>self.steps=[0, 20, 40, 60, 80,…,980] 表示每个20采样一个point，最终获得50维的向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, G, settings):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    This method </span><br><span class="line">    :param G: Input networkx graph object.</span><br><span class="line">    :param settings: argparse object with settings.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.index = G.nodes()</span><br><span class="line">    self.G = pygsp.graphs.Graph(nx.adjacency_matrix(G))  # 从邻接矩阵构建一个图</span><br><span class="line">    self.number_of_nodes = len(nx.nodes(G))  # 节点数 620</span><br><span class="line">    self.settings = settings</span><br><span class="line">    if self.number_of_nodes &gt; self.settings.switch:  # switch=100</span><br><span class="line">        self.settings.mechanism = &quot;approximate&quot;</span><br><span class="line">    # step_size=20, sample_number=50  self.steps=[0, 20, 40, 60, 80,...,980]</span><br><span class="line">    self.steps = [x * self.settings.step_size for x in range(self.settings.sample_number)]</span><br></pre></td></tr></table></figure><p>嵌入机制，如果节点数大于switch的话选approximate, 具体是什么意思，不懂。 现在选用的是approximate_structural_wavelet_embedding方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def create_embedding(self):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Depending the mechanism setting creating an exact or approximate embedding.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if self.settings.mechanism == &quot;exact&quot;:</span><br><span class="line">        self.exact_structural_wavelet_embedding()</span><br><span class="line">    else:</span><br><span class="line">        self.approximate_structural_wavelet_embedding()</span><br></pre></td></tr></table></figure><p>首先，对于一个无向图$G=(\mathcal{V}, \mathcal{E})$,它的拉普拉斯矩阵是$L=D-A$，其中$D$是$G$的度矩阵（一个对角阵，对角元素$D_{ii}$是节点$i$的度），$A$是$G$的邻接矩阵。 $L$的特征向量为$U$，特征值为$\Lambda=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{n}\right)$。</p><p>这一块代码的目的是pygsp.filters.Heat的源码公式如下：</p><script type="math/tex; mode=display">\hat{g}(x) = \exp \left( \frac{-\tau x}{\lambda_{\text{max}}} \right)</script><p>论文中公式为：</p><script type="math/tex; mode=display">g_s = e^{-\lambda s}</script><p>其中$s$是<strong>scaling parameter</strong>, $s$越大，能量传播越远，代码中为定制1000。heat_fliter是图$G$的<strong>heat kernel</strong>即热核特征。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def approximate_structural_wavelet_embedding(self):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Estimating the largest eigenvalue, setting up the heat filter and the Cheybshev polynomial. Using the approximate wavelet calculator method.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.G.estimate_lmax()  # 估计拉普拉斯算子的最大特征值, 结果被缓存在G.lmax()中。</span><br><span class="line">    # 公式：\hat&#123;g&#125;(x) = \exp \left( \frac&#123;-\tau x&#125;&#123;\lambda_&#123;\text&#123;max&#125;&#125;&#125; \right) 文中的 g_s(\lambda)</span><br><span class="line">    # tau: Scaling parameter tau控制能量传播距离，tau越大，能量传播的越远</span><br><span class="line">    self.heat_filter = pygsp.filters.Heat(self.G, tau=[self.settings.heat_coefficient])  # 热系数为1000</span><br><span class="line"></span><br><span class="line">    self.chebyshev = pygsp.filters.approximations.compute_cheby_coeff(self.heat_filter,</span><br><span class="line">                                                                      m=self.settings.approximation)  # shape:(101,)</span><br><span class="line">    self.approximate_wavelet_calculator()</span><br></pre></td></tr></table></figure><p>上面的代码不太懂什么意思。</p><p>下面的方法是计算每个节点的小波系数。</p><p>假设wavelet_coefficients 是节点$a$的小波系数即$\Psi_{a}$，$G$中有620个节点，那么wavelet_coefficients 是一个(620,)的向量，每个数$i$代表节点$a$从节点$i$接受到的能量。</p><p>如果两个节点具有相似的结构特征，那么他们的wavelet_coefficients 也应相似。</p><p>随后在0~1000以20的步长采样50个点：[0,20,40,…,980]，计算节点$a$的特征函数：</p><script type="math/tex; mode=display">\phi_{a}(t)=\frac{1}{N} \sum_{m=1}^{N} e^{i t \Psi_{m a}}</script><p>概率论中，任何随机变量的特征函数完全定义了它的概率分布。其中，$i$是虚部单位。$\Psi_{m a}$表示节点$a$和节点$i$的小波系数（即$a$从$m$获取的能量）。整个公式的意思本质上就是<strong>节点$a$的图谱小波(Spectral Graph Wavelets)$\Psi_{a}$在时间$t$采样的特征函数</strong> ，采样50次，那么就可以得到50个含$\phi_{a}$，将每个$\phi_{a}$的实部和虚部拼接就得到了最终的节点$a$的表示向量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def approximate_wavelet_calculator(self):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Given the Chebyshev polynomial, graph the approximate embedding is calculated. </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.real_and_imaginary = []  # 实部和虚部</span><br><span class="line">    for node in tqdm(range(0, self.number_of_nodes)):</span><br><span class="line">        impulse = np.zeros(self.number_of_nodes)  # 每个节点的响应</span><br><span class="line">        impulse[node] = 1  # 节点node的one-hot编码</span><br><span class="line"></span><br><span class="line">        # 节点node的小波系数 (620,) 表示节点node从每个节点接收到的能量分布</span><br><span class="line">        # 如果节点node和节点node1在网络中的结构相似，那么他们的小波系数(能量分布)也应相似</span><br><span class="line">        wavelet_coefficients = pygsp.filters.approximations.cheby_op(self.G, self.chebyshev, impulse)  # 节点node的小波系数</span><br><span class="line"></span><br><span class="line">        # 每隔20个点采样一次 共采样50次</span><br><span class="line">        # np.mean(np.exp(wavelet_coefficients * 1 * step * 1j)) 表示节点node的经验特征函数，1j是虚部单位</span><br><span class="line">        self.real_and_imaginary.append(</span><br><span class="line">            [np.mean(np.exp(wavelet_coefficients * 1 * step * 1j)) for step in self.steps])  # 论文中公式(2)</span><br><span class="line">    self.real_and_imaginary = np.array(self.real_and_imaginary)  # shape:(620,50)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 论文地址：&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3220025&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GraphWave&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参考：&lt;a href=&quot;https://zhuanl
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="Spectral graph wavelets" scheme="http://yoursite.com/tags/Spectral-graph-wavelets/"/>
    
      <category term="Heat kernel" scheme="http://yoursite.com/tags/Heat-kernel/"/>
    
  </entry>
  
  <entry>
    <title>《Graph Attention Networks》阅读笔记</title>
    <link href="http://yoursite.com/2019/04/14/GAT/"/>
    <id>http://yoursite.com/2019/04/14/GAT/</id>
    <published>2019-04-14T15:01:31.000Z</published>
    <updated>2019-04-15T14:00:34.720Z</updated>
    
    <content type="html"><![CDATA[<p> 论文地址：<a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">GAT</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文介绍了一种新型的神经网络架构用来处理图结构。即<strong>Graph Attention Networks</strong>(<strong>GATs</strong>)。该方法利用masked self-attentional layer，即通过网络层的堆叠，可以获取网络中每个节点的领域特征，同时为领域中的不同节点指定不同的权重。这样做的好处是可以不需要各种高成本的矩阵运算也不依赖于的图结构信息。通过这种方式，GAT可以解决基于谱的图神经网络存在的问题，同时，GAT可以使用用归纳（inductive）和转换（transductive）问题。</p><p><strong>归纳学习：</strong>先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的贝叶斯模型。</p><p><strong>转换学习：</strong>先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>图$G$中有$N$个节点，他们的特征向量为：$\textbf{h}=\{\vec{h_1},\vec{h_2},…,\vec{h_N}\}$，其中，$\vec{h_i} \in \mathbb{R}^F$，$F$是每个节点特征数。我们的目的是输出一个新的节点特征向量集$\textbf{h’}=\{\vec{h_1’},\vec{h_2’},…,\vec{h_N’}\}$，其中$\vec{h_i’} \in \mathbb{R}^{F’}$。 本质就是修改特征向量的维度（Network embedding）</p><p>为了获得足够的表达能力以将输入特征变换为更高级别的特征，需要至少一个可学习的线性变换。因此，以任意节点$i$和$j$为例，分别对节点$i$和节点$j$的特征向量做线性变换$W \in \mathbb{R}^{F \times F’}$，这样 就将$\vec{h_i}$和$\vec{h_j}$从$F$维的向量转化为$F’$维的向量：</p><script type="math/tex; mode=display">e_{ij} = a(W\vec{h_i},W\vec{h_j})</script><p>上式中，分别对$\vec{h_i}$和$\vec{h_j}$做线性变换，然后使用self-attention为图中的每一个节点分配注意力（权重）。上式中，注意力机制$a$是一个$\mathbb{R}^{F’} \times \mathbb{R}^{F’} \to \mathbb{R}$的映射。最终得到的$e_{ij}$是节点$j$对节点$i$的影响力系数（一个实数）。</p><p>但是，上面的方法考虑其他节点对$i$的影响时，将图中的所有节点都纳入了考虑范围，这样就丢失了图的结构信息。因此，本文引入<strong>masked attention</strong>机制，即计算影响力系数$e_{ij}$时， 仅考虑节点$i$的<strong>一部分邻居节点</strong> $j \in \mathcal{N}_i$（$i$也属于$\mathcal{N}_i$）。使用softmax将节点$i$部分邻居的注意力系数分配到(0,1)上：</p><script type="math/tex; mode=display">\alpha_{ij} = softmax_j(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i}\exp(e_{ik})}</script><p>在本文中，$a$是一个单层前馈神经网络，参数是一个权重向量$\vec{\text{a}} \in \mathbb{R}^{2F’}$，然后使用负半轴斜率为0.2的<a href="https://blog.csdn.net/sinat_33027857/article/details/80192789" target="_blank" rel="noopener">LeakyReLU</a>作为非线性激活函数：</p><script type="math/tex; mode=display">\alpha_{ij} = \frac{\exp(LeakyReLU(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i} \exp(LeakyReLU(\vec{\text{a}}^T[W\vec{h_i}||W\vec{h_k}]))}</script><p>其中$||$表示向量的连接操作。上述过程可以用下图表示：</p><p><img src="/2019/04/14/GAT/1.png" alt=""></p><p>这样，我们就可以获得节点$j$对节点$i$的注意力系数$\alpha_{ij}$，那么，节点$i$最终的输出特征$\vec{h_i’}$就是对$\mathcal{N}_i$中所有节点的加权（加注意力）求和：</p><script type="math/tex; mode=display">\vec{h_i'} = \sigma (\sum_{j \in \mathcal{N}_i}\alpha_{ij} W\vec{h_j})</script><p>另外，本文使用<strong>multi-head attention</strong>来稳定self-attention的学习过程，如下图所示： </p><p><img src="/2019/04/14/GAT/2.png" alt=""></p><p>图中是$K=3$ heads的multi-head attention，不同颜色的箭头表示一个独立的attention计算，每个邻居节点做三次attention计算。每次attention计算就是一个普通的self-attention，输出的结果是一个$\vec{h_i’}$。multi-head attention为每个节点$i$输出3个不同的$\vec{h_i’}$,，然后将这三个向量做连接或者取平均，得到最终的$\vec{h_i’}$：</p><script type="math/tex; mode=display">\vec{h_i'} = \|^K_{k=1} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)</script><p>上式为把不同$k$的向量做连接操作，其中$\alpha_{ij}^k$和$\mathbf{W}^{k}$表示第$k$个head的结果，我们可以注意到，最终输出的结果是$KF’$维的。除了concat之外，我们还可以通过求平均的方式来获得$\vec{h_i’}$:</p><script type="math/tex; mode=display">\vec{h'_i}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)</script><h1 id="Comparisions"><a href="#Comparisions" class="headerlink" title="Comparisions"></a>Comparisions</h1><ul><li><p>GAT是计算高效的。self-attention是在所有边上并行计算，并且输出的特征在所有边上并行计算，从而不需要昂贵的矩阵计算和特征分解。单个head的GAT的时间复杂度为$O\left(|V| F F^{\prime}+|E| F^{\prime}\right)$，其中$F$是输入的特征数，$|V|$和$|E|$分别是节点数和边数。复杂度与GCN相同。</p></li><li><p>与GCN不同的是，GAT为同一邻域中的节点分配不同的重要性（different importance），提升了模型容量。</p></li><li><p>注意机制以共享的方式应用于图中的所有边（共享$\mathbf{W}$），因此它不依赖于对全局图结构的预先访问或其所有节点的（特征）。这样有以下提升：</p><ul><li>不必是无向图。如果$i \to j$不存在,可以直接不用计算$\alpha_{ij}$。</li><li>可直接应用于归纳学习。</li></ul></li><li>GAT可以被描述为一种特殊的<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">MoNet(Geometric deep learning on graphs and manifolds using mixture model cnns)</a>。具体我还没看，不会<img src="/2019/04/14/GAT/com.gif" alt=""></li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">GAT layer</th><th style="text-align:center">t-SNE + Attention coefficients on Cora</th></tr></thead><tbody><tr><td style="text-align:center"><img src="http://www.cl.cam.ac.uk/~pv273/images/gat.jpg" alt=""></td><td style="text-align:center"><img src="http://www.cl.cam.ac.uk/~pv273/images/gat_tsne.jpg" alt=""></td></tr></tbody></table></div><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>参考：</p><p>GCN：<a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">https://arxiv.org/abs/1609.02907</a></p><p><a href="https://zhuanlan.zhihu.com/p/34232818" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34232818</a></p><p><a href="https://zhuanlan.zhihu.com/p/59176692" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59176692</a></p><p><a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">https://arxiv.org/abs/1710.10903</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 论文地址：&lt;a href=&quot;https://arxiv.org/abs/1710.10903&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GAT&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot;
      
    
    </summary>
    
      <category term="Graph Mining" scheme="http://yoursite.com/categories/Graph-Mining/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="Graph Neural Network" scheme="http://yoursite.com/tags/Graph-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV轮廓提取并计算图片中某一封闭区域的面积</title>
    <link href="http://yoursite.com/2019/04/02/Pic-closed-edge/"/>
    <id>http://yoursite.com/2019/04/02/Pic-closed-edge/</id>
    <published>2019-04-02T07:43:44.000Z</published>
    <updated>2019-04-02T08:35:06.360Z</updated>
    
    <content type="html"><![CDATA[<p>最近遇到一个问题，之前在<a href="https://zhuo931077127.github.io/2019/01/19/edge-detection-pj/" target="_blank" rel="noopener">这篇文章</a>中提到过一个边缘检测并且拟合直线的方法。但是如果需要计算区域内的面积，比如说要计算下图中类似三角形区域内的面积<br><img src="/2019/04/02/Pic-closed-edge/cut.jpeg" alt="你想输入的替代文字"><br>之前的做法是用Canny算子提取边缘，再用HoughLines拟合直线，然后求出交点坐标并计算三角形面积，其中，边缘提取后的图像如下图所示：<br><img src="/2019/04/02/Pic-closed-edge/edge.jpeg" alt="你想输入的替代文字"><br>我们可以很明显的看出这不是一个标准的三角形，所以如果想要更精确的获得三角形，就需要对图片进行轮廓提取，然后计算轮廓内区域的面积。这里给出代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># Input image</span><br><span class="line">img = cv2.imread(&apos;cut.jpeg&apos;, cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line"># Needed due to JPG artifacts</span><br><span class="line">_, temp = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)</span><br><span class="line"></span><br><span class="line"># Dilate to better detect contours</span><br><span class="line">temp = cv2.dilate(temp, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))</span><br><span class="line"></span><br><span class="line"># Find largest contour</span><br><span class="line">_, cnts, _ = cv2.findContours(temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)</span><br><span class="line">largestCnt = []</span><br><span class="line">for cnt in cnts:</span><br><span class="line">    if len(cnt) &gt; len(largestCnt):</span><br><span class="line">        largestCnt = cnt</span><br><span class="line"></span><br><span class="line"># Determine center of area of largest contour</span><br><span class="line">M = cv2.moments(largestCnt)</span><br><span class="line">x = int(M[&quot;m10&quot;] / M[&quot;m00&quot;])</span><br><span class="line">y = int(M[&quot;m01&quot;] / M[&quot;m00&quot;])</span><br><span class="line"></span><br><span class="line"># Initiale mask for flood filling</span><br><span class="line">width, height = temp.shape</span><br><span class="line">mask = img2 = np.ones((width + 2, height + 2), np.uint8) * 255</span><br><span class="line">mask[1:width, 1:height] = 0</span><br><span class="line"></span><br><span class="line"># Generate intermediate image, draw largest contour, flood filled</span><br><span class="line">temp = np.zeros(temp.shape, np.uint8)</span><br><span class="line">temp = cv2.drawContours(temp, largestCnt, -1, 255, cv2.FILLED)</span><br><span class="line">_, temp, mask, _ = cv2.floodFill(temp, mask, (x, y), 255)</span><br><span class="line">temp = cv2.morphologyEx(temp, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))</span><br><span class="line"></span><br><span class="line"># Count pixels in desired region</span><br><span class="line">area = cv2.countNonZero(temp)</span><br><span class="line"></span><br><span class="line"># Put result on original image</span><br><span class="line">img = cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, 255)</span><br><span class="line"></span><br><span class="line">cv2.imshow(&apos;Input&apos;, img)</span><br><span class="line">cv2.imshow(&apos;Temp image&apos;, temp)</span><br><span class="line"></span><br><span class="line">cv2.waitKey(0)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>最后我们可以得到一个比较准确的轮廓：<br><img src="/2019/04/02/Pic-closed-edge/img_trk.jpg" alt="你想输入的替代文字"><br>面积如图中所示：<br><img src="/2019/04/02/Pic-closed-edge/img_tr.jpg" alt="你想输入的替代文字">  </p><p>参考：<br><a href="https://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours" target="_blank" rel="noopener">https://stackoverflow.com/questions/55467031/how-to-get-the-area-of-the-contours</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近遇到一个问题，之前在&lt;a href=&quot;https://zhuo931077127.github.io/2019/01/19/edge-detection-pj/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这篇文章&lt;/a&gt;中提到过一个边缘检测并且拟合
      
    
    </summary>
    
      <category term="project" scheme="http://yoursite.com/categories/project/"/>
    
      <category term="Computer Vision" scheme="http://yoursite.com/categories/project/Computer-Vision/"/>
    
    
      <category term="Contours" scheme="http://yoursite.com/tags/Contours/"/>
    
  </entry>
  
  <entry>
    <title>社交网络影响最大化（Influence Maximization）中的IC，LT模型</title>
    <link href="http://yoursite.com/2019/03/20/IC-LT/"/>
    <id>http://yoursite.com/2019/03/20/IC-LT/</id>
    <published>2019-03-20T13:34:08.000Z</published>
    <updated>2019-03-20T14:58:39.229Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Independent-Cascade-Model-IC-Model"><a href="#The-Independent-Cascade-Model-IC-Model" class="headerlink" title="The Independent Cascade Model (IC Model)"></a>The Independent Cascade Model (IC Model)</h1><p>IC模型，即独立级联模型。 $v$表示图$G=（V,E)$中的一个节点（用户），$v$可以被它的传入邻居（incoming neighbor）以一个影响概率$p_{u,v}$的情况下独立激活。<br>在时间步为0的时候，给定一个种子集（seed set）$S$,即$k$个已经是激活着的节点。 在时间步$t$时，每一个激活状态的节点$u$将会以一定的概率$p_{u,v}$来激活每一个和它连接的未激活节点$v$。时间步$t$之后，如果$v$依然是未激活状态，那么$v$将无法再次被$v$激活。结束一次上述过程后，$u$保持激活状态并且失去激活其他节点的能力。当网络$G$中没有其他节点可以被激活时，扩散过程结束。<br>值得注意的是，当$S$是原始的激活节点集，那么上述随机激活过程完成后，所得到的影响分布（Influence Spread）是所期望的激活节点数。</p><h1 id="The-Linear-Threshold-Model-LT-Model"><a href="#The-Linear-Threshold-Model-LT-Model" class="headerlink" title="The Linear Threshold Model (LT Model)"></a>The Linear Threshold Model (LT Model)</h1><p>LT模型，即线性阈值模型。基本思想是，如果一个未被激活的节点有足够的传入邻居是激活状态的，那么该节点可以被激活。<br>形式上， 在图$G$中每条边$e=(u,v) \in E$有一个权重$b_{u,v}$。 我们定义$\mathcal{N}_I (v)$表示节点$v$的传入节点， 满足$\sum_{u \in \mathcal{N}_I (v)} b_{u,v} \leq 1$, 即所有$v$的传入节点与$v$组成的边的权重只和小于1。 另外，每个节点$v$具有一个阈值$\theta_v$。 LT模型首先为每个节点$v$的阈值$\theta_v$在$[0,1]$上均匀随机采样。在时间步0时，设置$S$中的节点状态为激活，其他节点为未激活，然后迭代的更新每个节点的状态。 在时间步$t$时，所有$t-1$时刻是激活状态的节点依旧保持激活状态，与此同时，其他未激活的节点$v$, 如果任意一个节点$v$的激活传入邻居总权重的值至少为$\theta_v$,那么将$v$激活。 当没有节点将要被激活时，传播结束。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;The-Independent-Cascade-Model-IC-Model&quot;&gt;&lt;a href=&quot;#The-Independent-Cascade-Model-IC-Model&quot; class=&quot;headerlink&quot; title=&quot;The Independent 
      
    
    </summary>
    
      <category term="Social Network Analysis" scheme="http://yoursite.com/categories/Social-Network-Analysis/"/>
    
      <category term="Influence Maximization" scheme="http://yoursite.com/categories/Social-Network-Analysis/Influence-Maximization/"/>
    
    
      <category term="IM" scheme="http://yoursite.com/tags/IM/"/>
    
  </entry>
  
  <entry>
    <title>《BiNE:Bipartite Network Embedding》阅读笔记</title>
    <link href="http://yoursite.com/2019/03/13/BiNE/"/>
    <id>http://yoursite.com/2019/03/13/BiNE/</id>
    <published>2019-03-13T14:01:15.000Z</published>
    <updated>2019-03-14T06:21:25.863Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/sigir18-bipartiteNE.pdf" target="_blank" rel="noopener">BiNE</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Bipartite Network(二分网络)</strong>:如下图所示：<br><img src="/2019/03/13/BiNE/stru.png" alt="你想输入的替代文字"><br>二分网络将节点分为两种类型，其中 边存在于两种类型之间，例如users-items组成的推荐网络，相同类型节点间不会产生边。传统的Network Embedding将bipartite network视为同质网络（homogeneous network）， 也就是仅考虑直接联系的边。这样就存在一个问题，即在同一个类型中的节点，虽然没有直接的连接，但是也可能有间接的关系。比如两个用户同时连接到同一个商品，则这两个用户可能有有相同的购买偏好。</p><p>另一个问题，<br><img src="/2019/03/13/BiNE/r1.png" alt="你想输入的替代文字"> <img src="/2019/03/13/BiNE/r2.png" alt="你想输入的替代文字"><br>如上两图所示， 图一可以看出，节点被访问的词数和考虑到的节点数呈现斜率为-1.582的幂律分布，但是基于随机游走的生成器相对于真实分布有所偏差， 文中分析原因在于基于随机游走的模型DeepWalk为每个节点生成相同长度的节点序列（walk_length）并且每个节点所需要的随机游走次数（walk per vertex）也是完全相同的，这样无法反应网络的特征以及异构性。<br>另外，对于Heterogeneous Network Embedding方法metapath2vec++, 此方法是次优的因为它将直接连接的节点与间接有关系的节点视为等价。</p><p>针对以上问题，BiNE为直接连接和间接关系分别设计了专用的目标函数，并且联合优化。 并且，所及游走的长度由该节点的重要程度决定，节点的重要程度通过<a href="http://www.cs.cornell.edu/home/kleinber/auth.pdf" target="_blank" rel="noopener">HITS</a>来衡量。</p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>如figure中的二分网络， 可以这样定义：$G=(U,V,E)$,和一个$|U| \times |V|$的$W=[w_{ij}]$为权重矩阵。输出d维embedding向量$U=[\overrightarrow{u_i}]$, $V=[\overrightarrow{v_i}]$，结构如下图所示：<br><img src="/2019/03/13/BiNE/3.png" alt="你想输入的替代文字">（取自作者的讲解ppt）  </p><h2 id="Explicit-Relations"><a href="#Explicit-Relations" class="headerlink" title="Explicit Relations"></a>Explicit Relations</h2><p>同LINE一样， 基于直接连接的目标函数表示为：  </p><script type="math/tex; mode=display">minimize \quad O_1=-\sum_{e_{ij} \in E}w_{ij}\log \hat{P}(i,j)</script><h2 id="Implicit-Relations"><a href="#Implicit-Relations" class="headerlink" title="Implicit Relations"></a>Implicit Relations</h2><h3 id="构造随机游走序列"><a href="#构造随机游走序列" class="headerlink" title="构造随机游走序列"></a>构造随机游走序列</h3><p>这是本文的创新点，分别为$U$和$U$构建语料库，即随机游走序列$D^U$和$D^V$。 首先给出两个同type节点相似度的定义：  </p><script type="math/tex; mode=display">w^U_{ij}=\sum_{k \in V}w_{ik}w_{jk}</script><script type="math/tex; mode=display">w^V_{ij}=\sum_{k \in U}w_{ki}w_{kj}</script><p>其中$i$和$j$是为$U$或$V$中的同类节点，也就是说两个同类节点如果有共同目标顶点，那么他们的2介相似度不为0。 则$U$中的二阶权重矩阵$|U|\times|U|$维矩阵$W^U=[w^U_{ij}]$。$V$中同理。<br><img src="/2019/03/13/BiNE/Al1.png" alt="你想输入的替代文字"><br>其中$l=max(H(v_i)\times maxT,minT)$, $H(v_i)$为节点$v_i$的中心性，中心性由HITS衡量。$l$为节点$v$的random walk次数（the number of random walks）,有这个节点的重要程度（centrality/importance）决定。  </p><script type="math/tex; mode=display">D_{v_i}=BiasedRandomWalk(W^R,v_i,p)</script><p>表示其中一次随机游走的节点集合$p$表示停止概率。</p><p>通过上面的推导，可以对分别对$U$,$V$中的节点构早随机游走序列，以$U$为例，若$S\in D^U$表示 那么$S$就是$U$中的一个随机游走序列。</p><h3 id="对间接关系建模"><a href="#对间接关系建模" class="headerlink" title="对间接关系建模"></a>对间接关系建模</h3><p>如下图所示（图片取自作者的ppt），$S$为$D^U$中的一个随机游走序列， 其中$u_i$是这个序列的中心点，$C_s(u_i)$是$u_i$的上下文节点。<br><img src="/2019/03/13/BiNE/dd.png" alt="你想输入的替代文字"><br>对于$U$中的随机游走序列结合$D^U$，我们要做的就是最大化给定$u_i$,生成$u_c \in C_s(u_i)$的条件概率。所以目标函数如下：</p><script type="math/tex; mode=display">maximize \quad O_2 = \prod_{u_i \in S \land S \in D^U} \prod_{u_c \in C_s(u_i)}P(u_c|u_i)</script><p>对于$D^V$同理。其中,$p(u_c|u_i) = \frac{\exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_c)}{\sum^{|U|}_{k=1} \exp(\overrightarrow{u}_i^T \overrightarrow{\theta}_k))}$。</p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>本文的负采样方法是基于局部敏感哈希（LSH）来对与中心节点不相似的节点进行采样。<br>该方法的strategy是，给定一个中心节点，随机选取一个bucket（序列）并且这个序列不包含给定的中心节点，以此来获得和给定节点尽量不相似的负采样节点。<br>$N^{ns}_S (u_i)$ 表示$ns$个负采样节点，对于中心节点$u_i$, 那么上文提到的条件概率$p(u_c|u_i)$可以被定义为下式：  </p><script type="math/tex; mode=display">p(u_c,N^{ns}_S (u_i)|u_i) = \prod_{z \in \{u_c\} \cup N^{ns}_S (u_i)} P(z|u_i)</script><p>其中条件概率$P(z|u_i)$定义为：<br><img src="/2019/03/13/BiNE/4.png" alt="你想输入的替代文字"><br>其中$\sigma$表示sigmoid函数，这样就减少了上文softmax函数造成的计算量过大的问题。</p><h2 id="联合优化"><a href="#联合优化" class="headerlink" title="联合优化"></a>联合优化</h2><p>通过随机梯度上升对3部分损失函数进行加权优化：  </p><script type="math/tex; mode=display">maximize \quad L = \alpha \log O_2+\beta \log O_3 - \gamma O_1</script><p>最终BiNE的整体算法流程如下：<br><img src="/2019/03/13/BiNE/Al2.png" alt="你想输入的替代文字">  </p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>这篇文章提出的分布式训练以及负采样策略还是很值得学习的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://www.comp.nus.edu.sg/~xiangnan/papers/sigir18-bipartiteNE.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BiNE&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;I
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="Bipartite Network" scheme="http://yoursite.com/tags/Bipartite-Network/"/>
    
      <category term="SGA" scheme="http://yoursite.com/tags/SGA/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的优化算法总结</title>
    <link href="http://yoursite.com/2019/02/28/Optimizer/"/>
    <id>http://yoursite.com/2019/02/28/Optimizer/</id>
    <published>2019-02-28T03:48:28.000Z</published>
    <updated>2019-03-18T06:02:49.100Z</updated>
    
    <content type="html"><![CDATA[<p>最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。</p><h2 id="Gradient-Desent-梯度下降"><a href="#Gradient-Desent-梯度下降" class="headerlink" title="Gradient Desent(梯度下降)"></a>Gradient Desent(梯度下降)</h2><p>目标函数$f(x)$，其中$x$为模型的待优化参数，对于每个epoch $t$, $\eta_t$表示第$t$个epoch的步长。$x_t$第$t$个epoch时的参数。<br>(1).梯度下降的原理：目标函数（损失函数）$f(x)$关于参数$x$的梯度是损失函数上升最快的方向。所以只要让$x$沿梯度的反方向走，就可以缩小目标函数。<br>(2).目标函数关于参数$x$在epoch $t$时的梯度：  </p><script type="math/tex; mode=display">g_t = \nabla_x f(x_t)</script><p>(3).我们要最小化$f(x)$, 所以参数$x$需要往梯度的反方向移动：  </p><script type="math/tex; mode=display">x_{t+1} = x_t-\eta_t g_t</script><p>其中$x_{t+1}$为$t+1$时刻的参数值。</p><h2 id="Stochastic-Gradient-Desent-随机梯度下降"><a href="#Stochastic-Gradient-Desent-随机梯度下降" class="headerlink" title="Stochastic Gradient Desent(随机梯度下降)"></a>Stochastic Gradient Desent(随机梯度下降)</h2><p>梯度下降存在的问题有鞍点问题以及无法找到全局最优解的问题。所以引入SGD。<br>首先给出无偏估计的定义，稍后会用到：<br><a href="https://www.cnblogs.com/notwice/p/8538539.html" target="_blank" rel="noopener">无偏估计</a>：估计量的均值等于真实值，即具体每一次估计值可能大于真实值，也可能小于真实值，而不能总是大于或小于真实值（这就产生了系统误差）。  </p><p>深度学习中，目标函数通常是训练集中各个样本损失的平均，假设一个batch的大小为$n$，那么训练这个batch的损失就是$f_{batch}(x) = \frac{\displaystyle\sum_{i=1}^{n} f_i(x)}{n}$ , 所以目标函数对$x$的梯度就是：  </p><script type="math/tex; mode=display">\nabla f_{batch}(x) = \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x)</script><p>如果使用GD来优化：  </p><script type="math/tex; mode=display">x_{t+1} = x_{t}- \eta_t \frac{1}{n} \displaystyle\sum_{i=1}^n \nabla f_i(x) \\ = x_t-\eta_t \nabla f_{batch}(x)</script><p>上式可以看出，当训练样本非常大时，n也将边的非常大，那么梯度计算的计算开销就比较大。</p><p>随机梯度下降（SGD）的思想是： 以一个batch为例，这个batch中有n个样本，每个样本$i \in \{1, \cdots,n\}$, 每次从中随机选取一个样本来更新参数$x$。  </p><script type="math/tex; mode=display">x_{t+1} = x_{t}-\eta_t \nabla f_i(x)</script><p>这样就更新了一个batch的参数。 对比上面两个式子可以看出SGD降低了计算复杂度。上面两个式子是等价的，因为随机梯度$\nabla f_i(x)$是对梯度$\nabla f_{batch}(x)$的无偏估计，因为：  </p><script type="math/tex; mode=display">E_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f_{batch}(\boldsymbol{x})</script><p>符合无偏估计的定义。</p><h2 id="Momentum-动量法"><a href="#Momentum-动量法" class="headerlink" title="Momentum(动量法)"></a>Momentum(动量法)</h2><h3 id="Exponentially-weighted-moving-averages-EMA"><a href="#Exponentially-weighted-moving-averages-EMA" class="headerlink" title="Exponentially weighted moving averages(EMA)"></a>Exponentially weighted moving averages(EMA)</h3><p>EMA,指数加权移动平均数。</p><p>在GD中,如果学习率过大，会导致目标函数发散，而无法逼近最小值，如下图所示：<br><img src="/2019/02/28/Optimizer/EMA_1.png" alt="你想输入的替代文字"><br>如果学习率很低，那么会缓慢接近最优点，如下图红色轨迹：<br><img src="/2019/02/28/Optimizer/EMA_2.png" alt="你想输入的替代文字"><br>我们希望在学习率较小的时候可以更快逼近最优点，在学习率大的时候自变量可以不发散，即在正确的方向上加速下降并且抑制震荡，也就是达到如下的效果：<br><img src="/2019/02/28/Optimizer/EMA_3.svg" alt="你想输入的替代文字">   </p><p>因此引入EMA。给定参数$0 \leq \gamma &lt; 1$,当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合。  </p><script type="math/tex; mode=display">y_t = \gamma y_{t-1} + (1-\gamma) x_t</script><p>展开上式:  </p><script type="math/tex; mode=display">\begin{split}\begin{aligned}y_t  &= (1-\gamma) x_t + \gamma y_{t-1}\\         &= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\\         &= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\\         &\ldots\end{aligned}\end{split}</script><p>上式可以看出当前时刻变量是对过去时刻变量做指数加权，离当前时刻越近，加权越大（越接近1）。<br>在现实中，我们将$y_t$看作是最近$1/(1-\gamma)$个时间步的$x_t$的加权平均，当$\gamma = 0.95$时，是最近20个时间步的$x_t$值的加权平均。当$\gamma=0.9$时,可以看做是最近10个时间步加权平均。</p><h3 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h3><script type="math/tex; mode=display">\begin{split}\begin{aligned}\boldsymbol{v}_t &= \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\\boldsymbol{x}_t &= \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,\end{aligned}\end{split}</script><p>其中$g_t = \nabla f_i(x)$上式可以看出，如果$\gamma=0$，则上式就是一个普通的随机梯度下降法。$0 \leq \gamma &lt; 1$. $\gamma$一般取0.9。<br>一般，初始化$v_0=0$, 则  </p><script type="math/tex; mode=display">v_1=\eta_t g_t \\ v_2=\gamma v_1+\eta_t g_t = \eta_t g_t(\gamma+1) \\ v_3 = \eta_t g_t (\gamma^2+\gamma+1) \\ v_{inf} = \frac{(\eta_t g_t)\cdot(1-\gamma^{inf+1})}{1-\gamma}\approx \frac{(\eta_t g_t)}{1-\gamma}</script><p>相比原始梯度下降算法，动量梯度下降算法有助于加速收敛。当梯度与动量方向一致时，动量项会增加，而相反时，动量项减少，因此动量梯度下降算法可以减少训练的震荡过程。</p><p>换种方式理解动量法：<br><img src="/2019/02/28/Optimizer/m.jpg" alt="你想输入的替代文字"><br>如上图所示，A点为起始点，首先计算A点的梯度$\nabla a$，下降到B点，  </p><script type="math/tex; mode=display">\theta_{new} = \theta-\eta\nabla a</script><p>其中$\theta$为参数， $\eta$为学习率<br>到达B点后要加上A点的梯度，但是A点的梯度有个衰减值$\gamma$,推荐取0.9，相当于加上一个来自A点递减的加速度。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：  </p><script type="math/tex; mode=display">v_t = \gamma v_{t-1}+\eta \nabla b</script><script type="math/tex; mode=display">\theta_{new} = \theta-v_t</script><p>其中$v_{t-1}$表示之前所有步骤累计的动量和，$\nabla b$为B点的梯度方向。这样一步一步下去，带着初速度的小球就会极速的奔向谷底。</p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>假设目标函数有两个参数分别为$x_1$,$x_2$,若梯度下降迭代过程中，始终使用相同的学习率$\eta$:  </p><script type="math/tex; mode=display">x_{1_{new}} = x_1-\eta \frac{\partial f}{\partial x_1}</script><script type="math/tex; mode=display">x_{2_{new}} = x_2-\eta \frac{\partial f}{\partial x_2}</script><p>AdaGard算法根据自变量在每个维度的梯度值来调整各个维度上的学习率，避免学习率难以适应维度的问题。adagrad方法是将每一个参数的每一次迭代的梯度取平方累加再开方，用基础学习率除以这个数，来做学习率的动态更新。<br>$\nabla_{\theta_i} J(\theta)$表示第$i$个参数的梯度，其中$\theta=(\theta_1,\theta_2,…)$有$n$个参数。如果使用SGD来优化第$i$个参数，我们可以表示为:  </p><script type="math/tex; mode=display">\theta_{i\_new} = \theta_i-\eta \nabla_{\theta_i}J(\theta)</script><p>如果使用Adagrad，则可以表示为这样:  </p><script type="math/tex; mode=display">\theta_{i,t+1}=\theta_{i,t}-\frac{\eta}{\sqrt{G_{i,t}+\epsilon}} \nabla_{\theta_{i,t}}J(\theta)</script><p>$i,t$ 表示优化参数$\theta_i$时的第$t$次迭代，$\epsilon$防止分母为0，可以取$10^{-6}$,$G_{i,t}$表示对参数$\theta_i$优化的前$t$步的梯度的累加：  </p><script type="math/tex; mode=display">G_{i,t} = G_{i,t-1}+\nabla_{\theta_{i,t}}J(\theta)</script><p>新公式可以简化成:  </p><script type="math/tex; mode=display">\theta_{t+1}= \theta_t-\frac{\eta}{\sqrt{G_t+\epsilon}}\nabla_{\theta_t}J(\theta)</script><p>可以从上式看出，随着迭代的推移，新的学习率$\frac{\eta}{\sqrt{G_t+\epsilon}}$在缩小，说明Adagrad一开始激励收敛，到了训练的后期惩罚收敛，收敛速度变慢</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>主要解决Adagrad学习率过快衰减问题，类似动量的思想，引入一个超参数，在积累梯度平方项进行衰减.  </p><script type="math/tex; mode=display">s = \gamma \cdot s +(1-\gamma) \cdot \nabla J(\theta) \odot \nabla J(\theta)</script><p>参数$\theta$的迭代目标函数可以改写为:  </p><script type="math/tex; mode=display">\theta_{new} = \theta - \frac{\eta}{\sqrt{s+\varepsilon}} \odot \nabla J(\theta)</script><p>可以看出$s$是梯度的平方的指数加权移动平均值，$\gamma$一般取0.9，有助于解决 Adagrad中学习率下降过快的情况。</p><h2 id="Adaptive-moment-estimation-Adam"><a href="#Adaptive-moment-estimation-Adam" class="headerlink" title="Adaptive moment estimation(Adam)"></a>Adaptive moment estimation(Adam)</h2><p>Adam可以说是用的最多的优化算法，Adam通过计算一阶矩估计和二阶矩估计为不同的参数设计独立的自适应学习率。</p><h2 id="Adabound"><a href="#Adabound" class="headerlink" title="Adabound"></a>Adabound</h2><p>正在学习中</p><p>参考文献：<br><a href="https://zhuanlan.zhihu.com/p/32626442" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32626442</a><br><a href="https://zhuanlan.zhihu.com/p/31630368" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31630368</a><br><a href="https://zh.gluon.ai/" target="_blank" rel="noopener">https://zh.gluon.ai/</a><br><a href="https://blog.csdn.net/tsyccnh/article/details/76270707" target="_blank" rel="noopener">https://blog.csdn.net/tsyccnh/article/details/76270707</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想好好学一学Deep Learning中的优化算法（不能一直Adam了），看了一些文献，用这篇文章做个总结笔记。&lt;/p&gt;
&lt;h2 id=&quot;Gradient-Desent-梯度下降&quot;&gt;&lt;a href=&quot;#Gradient-Desent-梯度下降&quot; class=&quot;heade
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://yoursite.com/categories/Deep-Learning/"/>
    
    
      <category term="Optimizer" scheme="http://yoursite.com/tags/Optimizer/"/>
    
  </entry>
  
  <entry>
    <title>《Self-Paced Network Embedding》阅读笔记[转+改]</title>
    <link href="http://yoursite.com/2019/02/18/SeedNE/"/>
    <id>http://yoursite.com/2019/02/18/SeedNE/</id>
    <published>2019-02-18T07:36:57.000Z</published>
    <updated>2019-02-20T08:28:18.343Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址: <a href="https://www.kdd.org/kdd2018/accepted-papers/view/self-paced-network-embedding" target="_blank" rel="noopener">SeedNE</a></p><p>部分内容转载自<a href="https://zhuanlan.zhihu.com/p/55104326" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/55104326</a></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>传统的NE方法例如DeepWalk, Node2vec, Line是通过将正上下文节点在低维空间中靠近锚节点(anchor node)，而负上下文节点在低维空间中远离锚节点来学习Network Representation.也就是说，在低维空间中拉近相似节点，同时推远不相似节点。当采样负上下文节点时，通常采用预定义基于节点流行程度的采样分布，越流行的节点越有可能被采样为负例。<br>在数学上，这类似一个二分类器。用来把两个相似的节点分类成正相关节点对或负相关节点对。但是随机负例采样的方式无法区别出不同节点的信息，而是将所有节点等价采样。所以，DeepWalk，node2vec,LINE采样负节点是根据一个分布，这个分布依赖于节点的度。因此，采样连边多的节点会得到更好的效果，因为连边多的节点包含更多信息。然而这种方法忽略了度少的节点也可能包含有用的信息。所以不能仅仅用连接信息来决定一个节点的信息量。因此，本文的目标是找到真正有用的节点来训练模型。<br>当前的NE方法所使用的的采样方式是静态的，即采样分布不会随着训练过程而变化。所以在训练模型中，一个节点的信息量是个常量。但是，随着训练过程的继续，一些负例节点会远离锚节点，然而有一些却不会。所以我们应该更加注意这些不好区分的节点。因此，最好在不同的训练阶段对不同概率的节点进行采样。另一方面，不能总是关注不好区分的节点。应该从易到难的采样。</p><p>因此，本文的目标是动态地为训练模型抽取信息负面节点。具体来说，本文提出了一种自步节点采样策略(self-paced)。 该策略可以基于当前模型参数发现每个节点的信息量，然后根据其信息量对负节点进行采样。 此外，这种自定进度的策略可以在训练过程中逐渐对困难的负面节点进行采样。 此外，本文将这种自我采样策略扩展到生成对抗性网络框架。 对七个基准网络数据集进行了大量实验，以验证提出的方法的有效性。</p><h1 id="Sample-Selection"><a href="#Sample-Selection" class="headerlink" title="Sample Selection"></a>Sample Selection</h1><p>Curriculum Learning(课程学习)：<a href="https://blog.csdn.net/qq_25011449/article/details/82914803" target="_blank" rel="noopener">解读</a><br>Self-Paced Learning(自步学习)：<a href="https://blog.csdn.net/selous/article/details/78144377" target="_blank" rel="noopener">解读</a></p><h1 id="Self-Paced-Network-Embedding"><a href="#Self-Paced-Network-Embedding" class="headerlink" title="Self-Paced Network Embedding"></a>Self-Paced Network Embedding</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址: &lt;a href=&quot;https://www.kdd.org/kdd2018/accepted-papers/view/self-paced-network-embedding&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SeedNE&lt;/a&gt;&lt;/
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="Self-Paced" scheme="http://yoursite.com/tags/Self-Paced/"/>
    
  </entry>
  
  <entry>
    <title>《GraphGAN:Graph Representation Learning with Generative Adversarial Nets》阅读笔记</title>
    <link href="http://yoursite.com/2019/01/22/GraphGAN/"/>
    <id>http://yoursite.com/2019/01/22/GraphGAN/</id>
    <published>2019-01-22T08:57:05.000Z</published>
    <updated>2019-01-24T09:19:46.165Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1711.08267" target="_blank" rel="noopener">GraphGAN</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>GAN是CV上非常火的方法，作者将其引入Network Embedding领域。为了达到这个目的，首先，作者将当前的NE model分成了两类，分别是生成模型(generative model)和判别模型(discriminative model)。简单来说，生成模型。<br>所谓生成模型，就是对于每个节点$v_c$，都存在一个潜在的真实连接分布$p_{true}(v|v_c)$, 这个分布反映了由条件生成样本的概率，因此，生成式模型的目的就是求网络中边的极大似然。比较经典的方法有DeepWalk，是通过随机游走来获得center node的上下文，然后最大化上下文节点的似然。Node2vec拓展了DeepWalk,提出了biased随机游走，使得模型在为给定节点生成上下文节点时具有更多的灵活性。<br>所谓判别模型，目的是学习一个分类器来直接预测边是否存在。将两个节点视为输入特征，然后输出预测的两个节点之间有边的概率，即$p(edge|(v_i,v_j))$,常见的方法有SDNE，PPNE。</p><p>于是，本文结合生成对抗网络(GAN),提出了GraphGAN来同一生成和判别器。其中生成器$G(v|v_c)$试图拟合真实的连接概率分布$p_{true}(v|v_c)$,生成最可能和$v_c$有链接的点。判别器$D(v,v_c)$尝试区分强连接节点对和若连接节点对，然后计算$v$和$v_c$存在边的可能性。简单来说，就是把生成器生成的可能与$v_c$有边的节点放入判别器计算两者有边的概率。</p><p>除此之外GAN框架下，然而生成器的连接分布是通过softmax实现的，但是传统的softmax无法适配生成器，原因如下:<br>(1).传统的softmax对网络中的所有节点一视同仁，缺乏对网络结构相似度信息的考虑。<br>(2).计算成本太高。<br>因此，论文中提出了新的生成器实现方法 Graph Softmax。并且证明GS具有归一化（normalization）、网络结构感知（graph structure awareness）和高计算效率（computational efficiency）的性质。相应的，文中也提出了基于随机游走(random walk)的生成器策略。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>这里挑特别的来说。$\mathcal{N}(v_c)$表示和$v_c$直接相连的节点集。我们把给定节点$v_c$与任意节点$v \in \mathcal{V}$的真实连接分布表示为$p_{true}(v|v_c)$。这么看$\mathcal{N}(v_c)$可以看做从$p_{true}(v|v_c)$中采样的集合。文章的目的是学习两个模型:<br><strong>Generator</strong> $G(v|v_c;\theta_G)$ 生成器，尝试拟合真实连接分布$p_{true}(v|v_c)$，并且从节点集$\mathcal{V}$中生成最有可能和$v_c$相连的节点。<br><strong>Discriminator</strong> $D(v,v_c;\theta_G)$ 判别器，目的是判断已对接点$(v,v_c)$的连接性。$D(v,v_c;\theta_G)$输出$v$和$v_c$有边的概率。</p><p>生成器$G$尝试完美拟合$p_{true}(v|v_c)$，并且生成和$v_c$真实邻居高度相似的节点来欺骗判别器。相反，判别器尝试发现输入的顶点是真实的节点对或者有生成器生成出的样本。这是一个极大极小游戏，目标函数$V(G,D)$:  </p><script type="math/tex; mode=display">\min_{\theta_G} \max_{\theta_D} V(G,D)=\sum_{c=1}^V (\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]+\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))])</script><p>上面这个公式是本文最关键的公式，以我的分析就是：在给定$\theta_D$的情况下，对其最小化。先来分析$\max_{\theta_D}V(G,D)$,即给定$\theta_G$,使原式最大。当给定$\theta_G$时，通过改变$\theta_D$,使$\mathbb{E}_{v \sim p_{true}(\cdot|v_c)}[\log D(v,v_c;\theta_D)]$达到最大（是判别器D拟合真实分布），同时使$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$最大，即尽可能减小选定节点$v_c$与判别器中生成的$v_c$邻居节点与$v_c$连接的可能性。然后在给定$\theta_D$的情况下，通过改变生成器$\theta_G$继续生成节点，使得$\mathbb{E}_{v \sim G(\cdot|v_c;\theta_G)}[\log(1-D(v,v_c;\theta_D))]$尽可能小，即上一步的判别器尽可能把当前生成器生成的节点认为是邻居。 之后再更新判别器,以此类推。这样$G$和$D$各自提高性能，最终$G$生成的分布和真实连接分布无法区分，这样达到最好的效果。整体过程如下图所示:  </p><p><img src="/2019/01/22/GraphGAN/1.png" alt="你想输入的替代文字"></p><h3 id="Discriminator-Optimization"><a href="#Discriminator-Optimization" class="headerlink" title="Discriminator Optimization"></a>Discriminator Optimization</h3><p>对于判别器来说是个简单的sigmoid函数，来计算两个输入节点的表示向量:  </p><script type="math/tex; mode=display">D(v,v_c;\theta_D)=\frac{1}{1+\exp(-d^\top_v d_{v_c})}</script><p>其中，$d_v$,$d_{v_c}$是两个输入节点关于判别器$D$的表示向量，$\theta_D$是所有节点表示向量的结合。注意到上面的公式只涉及$v$和$v_c$, 我们只需要更新$d_v$,$d_{v_c}$，通过梯度下降的方法可以实现：<br><img src="/2019/01/22/GraphGAN/2.png" alt="你想输入的替代文字"></p><h3 id="Generator-Optimization"><a href="#Generator-Optimization" class="headerlink" title="Generator Optimization"></a>Generator Optimization</h3><p>对于生成器来说，目标是最小化判别器将生成的样本判断为负样本的对数似然（概率）。换句话说，生成器会调整自己的连接分布（通过调整生成的所有节点的向量表示$\theta_G$）来提升对生成样本的判别分数。由于$v$的采样时离散的，所以使用policy gradient来计算$V(G,D)$关于$\theta_G$的梯度：<br><img src="/2019/01/22/GraphGAN/3.png" alt="你想输入的替代文字"><br>为了理解上述公式，注意到$\nabla_{\theta_G}V(G,D)$是一个由$\log(1-D(v,v_c;\theta_D))$加权的梯度$\nabla_{\theta_G}\log G(v|v_c;\theta_G)$的求和，直观上说，这意味着有高概率是负样本的节点会将生成器G“拉着”远离自己（因为我们在$\theta_G$上执行梯度下降）。</p><h3 id="Graph-Softmax"><a href="#Graph-Softmax" class="headerlink" title="Graph Softmax"></a>Graph Softmax</h3><p>graph softmax的核心思想是定义一种新的计算连接性概率的方式，满足以下性质:  </p><ul><li>归一化：$\sum_{v \neq v_c;\theta_G}=1$。</li><li>图结构感知：生成器充分利用网络中的结构信息，来估计真实连接分布，如果两个节点在图上越远，那么他们间有边的概率越小。</li><li>高效的计算：和传统的softmax不同，$G(v|v_c;\theta_G)$的计算应只涉及图中的一小部分点。</li></ul><p>因此，本文以图中节点$v_c$为例，以$v_c$为根构建一棵BFS树$T_c$。$\mathcal{N}_c(v)$为节点$v$在$T_c$上的邻居集合（包括他的父节点和所有子节点）。对于一个given vertex $v$和它的一个邻居$v_i \in \mathcal{N}_c(v)$,定义概率为:  </p><script type="math/tex; mode=display">p_c(v_i|v)=\frac{\exp (g_{v_i}^\top g_v)}{\sum_{v_i \in \mathcal{N}_c(v)} \exp(g_{v_j}^\top g_v)}</script><p>这是一个在$\mathcal{N}_c(v)$上的softmax函数。</p><p>为了计算$G(v|v_c;\theta_G)$,注意到在$T_c$上，根节点$v_c$到每个节点$v$都有一条唯一的路径， 把这条路径记为$P_{v_c \to v}=(v_{r_0},v_{r_1},…,v_{r_m})$,其中$v_{r_0}=v_c$, $v_{r_m}=v$,那么在graph softmax中，将$G(v|v_c;\theta_G)$定义为:  </p><script type="math/tex; mode=display">G(v|v_c;\theta_G)\triangleq (\prod^m_{j=1} p_c(v_{r_j}|v_{r_{j-1}})) \cdot p_c(v_{r_{m-1}}|v_{r_m})</script><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>总的来说，这篇paper应该属于较早将GAN结合到NRL上的尝试， 有不少值得学习的地方，但是从实验部分看，Baselines较少以及试验指标选取中可以看出还有很大的提升空间。该算法主要针对无权图，对于加权图来说并不适配。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址:&lt;a href=&quot;https://arxiv.org/abs/1711.08267&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GraphGAN&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduct
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="Graph Softmax" scheme="http://yoursite.com/tags/Graph-Softmax/"/>
    
  </entry>
  
  <entry>
    <title>《Enhanced Network Embeddings via Exploiting Edge Labels》阅读笔记</title>
    <link href="http://yoursite.com/2019/01/22/NE-Edge-Labels/"/>
    <id>http://yoursite.com/2019/01/22/NE-Edge-Labels/</id>
    <published>2019-01-22T03:02:29.000Z</published>
    <updated>2019-01-22T06:31:16.548Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址: <a href="https://arxiv.org/abs/1809.05124?context=physics.soc-ph" target="_blank" rel="noopener">Enhanced Network Embeddings via Exploiting Edge Labels</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>这是DeepWalk团队的一篇论文，目的是捕获网络中的边信息。传统的NE方法通常把节点简单关系当做（0,1）二值，然而边中所包含的丰富的语义信息。本文尝试做Network Embedding的同时保留网络结构和节点关系信息。举个例子来说，真实社交网络中，一个用户可能和他的同事，家人有关系，但是已有的NE方法不能同事捕获好友关系（有边连接），以及边的类型。</p><p>具体来说，本分的方法分为无监督部分和监督部分。其中无监督部分预测节点邻域， 监督部分预测边标签。所以本文模型是个<strong>半监督NE模型</strong>。</p><h2 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h2><p>假定Network Graph $G=(V,E)$是无向图。$L=(l_1,l_2,…,l_{|V|})$是边的类型集。一个含有边类型的Graph可以被重新定义为$G=(V,E_L,E_U,Y_L)$,其中$E_L$是由label的边集，$E_U$是没有label的边集，$E_L \cup E_U =E$。$Y_L$表示$E_L$中边的关系类型集合。论文中假定一条边可以有多重关系，所以对于边$E_i$的label集$Y_L(i) \in Y_L$, $Y_L(i)$可能包含很多类型 所以$Y_L(i) \subseteq L$。目的还是一样，学习一个映射函数$\Phi \to \mathbb{R}^{|V| \times d}$, 其中$d \ll |V|$。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>首先定义损失函数:  </p><script type="math/tex; mode=display">\mathcal{L}=(1-\lambda)\mathcal{L}_s+\lambda\mathcal{L}_r</script><p>其中$\mathcal{L}_s$表示预测节点邻域的损失。$\mathcal{L}_r$表示预测边label的损失。$\lambda$是两种损失的权重。</p><h3 id="Structural-Loss"><a href="#Structural-Loss" class="headerlink" title="Structural Loss"></a>Structural Loss</h3><p>第一部分是最小化无监督网络结构损失，对于一个给定的节点$v$, 要最大化这个节点和它邻域可能性。其中，节点的邻域不一定要一定和该节点有边相连。文章先给出了结构损失的目标函数：  </p><script type="math/tex; mode=display">\mathcal{L}_s=-\sum_{u \in C(v)} \log Pr(u|v)</script><p>这个函数其实就是给定$v$,最大化$v$的邻域的极大似然。其中$Pr(u|v)$是一个softmax函数：  </p><script type="math/tex; mode=display">Pr(u|v)=\frac{\exp(\Phi(u) \cdot \Phi'(v))}{\sum_{u' \in V} \exp(\Phi(u') \cdot \Phi'(v))}</script><p>这其实和DeepWalk一样，一个节点$v$有两个表示向量，$\Phi(v)$和$\Phi’(v)$分别表示该节点作为中心节点和上下文节点的表示。由于计算复杂度较高，所以采用负采样的策略。<br>剩下的问题就是如何构建节点$v$的邻域$C(v)$。一种直接的方式就是从邻接矩阵中选取他的邻居。然后由于现实网络的稀疏性，一个节点只有很少的邻居。为了缓解网络稀疏性的问题， 本文采取了类似于DeepWalk的randomwalk策略。 最终可以得到节点$v$的邻域：  </p><script type="math/tex; mode=display">C(v)=\{v_{i-w},...,v_{i-1}\} \cup \{v_{i+1},...,v_{i+w}\}</script><h3 id="Relational-Loss"><a href="#Relational-Loss" class="headerlink" title="Relational Loss"></a>Relational Loss</h3><p>由于label是为了预测边的，所以需要把每条边表示出来，所以对于边$e=(u,v) \in E$,可以用一下方法来表示这条边:  </p><script type="math/tex; mode=display">\Phi(e)=g(\Phi(u),\Phi(v))</script><p>其中，$g$是一个映射函数用来把两个节点的表示向量转化为他们之间边的表示向量，本文使用了简单的连接操作，即把两个向量直接拼接：  </p><script type="math/tex; mode=display">\Phi(e)=\Phi(u) \oplus \Phi(v)</script><p>这样我们就获得了edge embedding。直接将它输入前馈神经网络，前馈神经网络第$k$层的定义为:  </p><script type="math/tex; mode=display">h^{(k)}=f(W^{(k)}h^{(k-1)}+b^{(k)})</script><p>其中 $h^{(0)}=\Phi(e)$，$f$是除最后一层外采用relu激活函数，最后一层采用sigmoid函数激活,最后一层输出为$\hat{y_i}$。最后最小化二元交叉熵损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_r=\sum^{|L|}_{i=1} H(y_i,\hat{y_i}) + (1-y_i) \cdot \log (1-\hat{y_i})</script><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>这篇论文从原理到方法实现都非常简单，稍后我也将尝试复现这篇论文，边的标签信息是以前NE方法所没有考虑到的，但这篇论问的局限性是没有考虑边的方向以及权重，这是可以拓展的方向。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址: &lt;a href=&quot;https://arxiv.org/abs/1809.05124?context=physics.soc-ph&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Enhanced Network Embeddings via Ex
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="Hawkes process" scheme="http://yoursite.com/tags/Hawkes-process/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>SDNE:《Structral Deep Network Embedding》阅读笔记</title>
    <link href="http://yoursite.com/2019/01/21/SDNE/"/>
    <id>http://yoursite.com/2019/01/21/SDNE/</id>
    <published>2019-01-21T07:34:36.000Z</published>
    <updated>2019-01-22T02:56:11.352Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf" target="_blank" rel="noopener">SDNE</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>这是一篇比较早的Network Embedding论文， 较早将深度模型应用到NE任务。 首先，本文提出了当前网络表示学习中遇到的三个问题：<br><strong>（1）. 高度非线性</strong><br><strong>（2）. 尽可能保持网络结构</strong><br><strong>（3）. 现实网络的高度稀疏性</strong><br>SDNE的主要目标就是保持网络结构的一阶相似性和二阶相似性。<br>一阶相似性就是网络中边相连的节点对之间具有的相似性。<br>二阶相似性就是在一个Graph中，拥有共同邻居但是不直接向相连的两个节点具有的相似性。</p><p>其中，一阶相似性主要反映了网络的局部特征。 二阶相似性反映了网络的全局特征。</p><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>本文的模型主要如下图所示：</p><p><img src="/2019/01/21/SDNE/SDNE.png" alt="你想输入的替代文字"></p><p>这张图看上去有点复杂，实则原理非常简单。</p><p>模型分为无监督部分和有监督部分，无监督部分是一个<strong>深度自编码器</strong> 用来捕获二阶相似度（保留全局结构），监督部分是一个拉普拉斯特征映射捕获一阶相似度（局部结构）。呃呃呃，Emmmmm ,不知道是我理解有问题还是其他原因，文章里说的和我理解的不太一样 /(ㄒoㄒ)/~~。 然后介绍一下具体的模型结构：  </p><p>深度自编码器的编码部分：  </p><script type="math/tex; mode=display">y_i^{(k)}=\sigma{(W^{(k)}y_i^{(k-1)}+b^{(k)})}, k=2,...,K</script><p>假设第$k$层是 节点$v$的表示向量（仅考虑全局信息），那么从第$k$层开始解码，最终得到$\hat{x_i}$, 所以自编码器的误差就是输入节点$v$的邻接向量的重构误差。所以，二阶相似度损失函数定义为:  </p><script type="math/tex; mode=display">\mathcal{L}=\sum_{i=1}^n{||\hat{x_i}-x_i||^2_2}</script><p>值得注意的是，由于网络的稀疏性，邻接矩阵中的0元素远多于非0元素，使用邻接矩阵作为输入的话要处理很多0，这样就做了太多无用功了。为了解决这个问题，对损失函数做了改进如下：  </p><script type="math/tex; mode=display">\mathcal{L_{2nd}}=\sum_{i=1}^n||(\hat{x_i}-x_i)\odot{b_i}||^2_2=||\hat{X}-X\odot{B}||^2_F</script><p>其中$\odot$是哈马达乘积，表示对应元素相乘。$b_i=\{b_{i,j}\}^n_{j=1}$， 邻接矩阵中的0对应$b=1$, 非0元素的$b&gt;1$,这样的目的是对于有边连接的节点增加惩罚。可以理解为对有边连接的节点赋予更高权重。</p><p>以上我们获得了二阶相似度的损失函数。在介绍一阶相似度之前，我们先来看看<strong>拉普拉斯映射（Laplacian Eigenmap）</strong>  其实LE也是一种经典的NRL方法，主要目的也是降维。其目标函数如下所示:  </p><script type="math/tex; mode=display">\sum_{i,j} W_{ij}||y_i-y_j||^2</script><p>LE是通过构建相似关系图来重构局部特征结构,如果放在网络结构中来说,如果节点$v_i$和$v_j$很接近（有边），那么他们在embedding space中的距离也应该相应接近。$y_i$和$y_j$就表示他们在特征空间中的表示。因此，本文定义了保持一阶相似度的目标函数：  </p><script type="math/tex; mode=display">\mathcal{L_{1st}}=\sum_{i,j=1}^n{s_{i,j}||y_i^{(K)}-y_j^{(K)}||^2_2}=\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}</script><p>具体来说，$K$就是自编码器第$K$层的输出，即编码结果，需要保持一条边的两个节点在嵌入空间中的表示相对接近。</p><p>最终 结合一阶相似度和二阶相似度，本文给出了SDNE的目标函数：  </p><script type="math/tex; mode=display">\mathcal{L_{mix}}=\mathcal{L_{2nd}+\alpha{\mathcal{L_{1st}}}}+\nu{\mathcal{L_{reg}}} =||(\hat{X}-X)\odot{B}||^2_F+\alpha{\sum_{i.j=1}^n{s_{i,j}||y_i-y_j||^2_2}}+\nu{\mathcal{L_{reg}}}</script><p>其中，为了防止过拟合，添加了$\mathcal L2$-norm单元$\mathcal{L_{reg}}$来防止过拟合:  </p><script type="math/tex; mode=display">\mathcal{L_{reg}}=\frac{1}{2}\sum_{k=1}^k({||W^{(k)}||^2_F+||\hat{W}^{k}||_F^2})</script><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>使用SGD来优化$\mathcal{L_{mix}}$。具体算法如下：<br><img src="/2019/01/21/SDNE/al.png" alt="你想输入的替代文字"></p><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>不得不感叹这篇论文的实验是真的充分，分别在Link Prediction，Vertex Classification，Visualization上做了评价，并且都取得了高于Baselines的效果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SDNE&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduc
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="semi-supervised model" scheme="http://yoursite.com/tags/semi-supervised-model/"/>
    
  </entry>
  
  <entry>
    <title>图片边缘检测并且拟合直线</title>
    <link href="http://yoursite.com/2019/01/19/edge-detection-pj/"/>
    <id>http://yoursite.com/2019/01/19/edge-detection-pj/</id>
    <published>2019-01-18T16:00:00.000Z</published>
    <updated>2019-01-19T04:11:50.156Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间做了一个简单的小项目，需求是测量图片中布的褶皱角度，第一次做CV的东西，决定用这个blog记录一下。 项目源码在:<a href="https://github.com/zhuo931077127/edge-detection" title="edge-detection" target="_blank" rel="noopener">edge-detection</a></p><p>先看看传统的边缘检测方法的效果:</p><p><img src="/2019/01/19/edge-detection-pj/tra.png" alt="你想输入的替代文字"></p><p>第一张图是原始图，由于本项目要求竟要求图片上半部分的褶皱角度，所以仅考虑上半部分的背景干扰，可以看出高斯，梯度，非极大抑制这三种方法都无法有效的排除干扰。<br>然后我们用canny算法试试。<br>步骤是先把图片转化为灰度图，然后用canny算子做边缘检测。<br>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)</span><br><span class="line">edges = cv.Canny(gray, 50, 310)  # apertureSize参数默认其实就是3  # 50 310</span><br><span class="line"># cv.imshow(&quot;edges&quot;, edges)</span><br><span class="line">edge = Image.fromarray(edges)</span><br><span class="line">edge.save(&quot;edge.jpeg&quot;)</span><br></pre></td></tr></table></figure></p><p>结果如下:  </p><p><img src="/2019/01/19/edge-detection-pj/edge.jpeg" alt="你想输入的替代文字"></p><p>霍夫线性变换拟合直线:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">lines = cv.HoughLines(edges, 1, np.pi / 180, 68)  # 68</span><br><span class="line">   # l1 = lines[:, 0, :]</span><br><span class="line">   # print(l1)</span><br><span class="line">   mink = float(&apos;inf&apos;)</span><br><span class="line">   maxk = -float(&apos;inf&apos;)</span><br><span class="line">   for line in lines:</span><br><span class="line">       rho, theta = line[0]  # line[0]存储的是点到直线的极径和极角，其中极角是弧度表示的。</span><br><span class="line">       a = np.cos(theta)  # theta是弧度</span><br><span class="line">       b = np.sin(theta)</span><br><span class="line">       x0 = a * rho  # 代表x = r * cos（theta）</span><br><span class="line">       y0 = b * rho  # 代表y = r * sin（theta）</span><br><span class="line">       x1 = int(x0 + 1000 * (-b))  # 计算直线起点横坐标</span><br><span class="line">       y1 = int(y0 + 1000 * a)  # 计算起始起点纵坐标</span><br><span class="line">       x2 = int(x0 - 1000 * (-b))  # 计算直线终点横坐标</span><br><span class="line">       y2 = int(y0 - 1000 * a)  # 计算直线终点纵坐标    注：这里的数值1000给出了画出的线段长度范围大小，数值越小，画出的线段越短，数值越大，画出的线段越长</span><br><span class="line">       print(&quot;x1: %s, y1:%s, x2:%s, y2:%s&quot; % (x1, y1, x2, y2))</span><br><span class="line">       k = (y2 - y1) / (x2 - x1)</span><br><span class="line">       if k &gt; maxk:</span><br><span class="line">           maxk = k</span><br><span class="line">           xmax1 = x1</span><br><span class="line">           ymax1 = y1</span><br><span class="line">           xmax2 = x2</span><br><span class="line">           ymax2 = y2</span><br><span class="line">           lineMax = line</span><br><span class="line">       if k &lt; mink:</span><br><span class="line">           mink = k</span><br><span class="line">           xmin1 = x1</span><br><span class="line">           ymin1 = y1</span><br><span class="line">           xmin2 = x2</span><br><span class="line">           ymin2 = y2</span><br><span class="line">           lineMin = line</span><br><span class="line">   cv.line(image, (xmax1, ymax1), (xmax2, ymax2), (255, 0, 0), 2)  # 点的坐标必须是元组，不能是列表。</span><br><span class="line">   cv.line(image, (xmin1, ymin1), (xmin2, ymin2), (255, 0, 0), 2)  # 点的坐标必须是元组，不能是列表。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>值得注意的是，拟合直线过程中HoughLines会发现很多直线，因此，我选择了斜率最大和最小的两条直线做为最终的直线。画在图上的话就是酱紫的结果:</p><p><img src="/2019/01/19/edge-detection-pj/line.png" alt="你想输入的替代文字"></p><p>所以：总的过程可以概括如下:</p><p><img src="/2019/01/19/edge-detection-pj/final.jpeg" alt="你想输入的替代文字"></p><p>现在想这么弱智的项目居然还花了几天时间做，我是真滴蠢哦（T-T）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前段时间做了一个简单的小项目，需求是测量图片中布的褶皱角度，第一次做CV的东西，决定用这个blog记录一下。 项目源码在:&lt;a href=&quot;https://github.com/zhuo931077127/edge-detection&quot; title=&quot;edge-detect
      
    
    </summary>
    
      <category term="project" scheme="http://yoursite.com/categories/project/"/>
    
      <category term="Computer Vision" scheme="http://yoursite.com/categories/project/Computer-Vision/"/>
    
    
      <category term="Edge detection" scheme="http://yoursite.com/tags/Edge-detection/"/>
    
  </entry>
  
  <entry>
    <title>HTNE:《Embedding Temporal Network via Neighborhood Formation》阅读笔记</title>
    <link href="http://yoursite.com/2019/01/17/HTNE/"/>
    <id>http://yoursite.com/2019/01/17/HTNE/</id>
    <published>2019-01-16T16:00:00.000Z</published>
    <updated>2019-01-19T13:37:37.503Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://dl.acm.org/citation.cfm?id=3220054" target="_blank" rel="noopener">HTNE</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文出发点在于捕获动态网络中节点和边的变化来在embedding中保持网络结构，举个简单的例子来说，如Fig. 1所示，是一个共同作者网络图。数字标注的节点是author，方框内是co-authored paper. 可以看到图中每个节点每条边加入网络中的时间是不同的。根据图中的信息，可以分析出例如前期1和2，3合作较紧密，后期转为了6,7。 并且(b)中所示，同一条边可能多次出现，这就比传统的单条边拥有更多语义信息。</p><p><img src="/2019/01/17/HTNE/Fig1.png" alt="你想输入的替代文字"></p><p>另外，最近也有方法对动态网络的embedding做了研究，比如[29][30]的方法。但是他们的目的是将时间线分段为固定时间窗来对动态建模，但是这些方法依然没有考虑动态过程也就是网络随时序动态变化的信息。</p><p>因此，本文提出了基于霍克斯过程（Hawkes process）的时序网络表示学习方法，该方法是由序列事件驱动的（也就是序列的变化） 如Fig .1(b)所示 (b)图为节点1的邻域生成序列。霍克斯过程的思路是说历史上发生的事情对未来的概率密度函数有影响，只是随着时间流逝这种影响会逐渐减弱（Decay）。本文提出用霍克斯过程来捕获时间序列（也就是邻域生成序列）的激励效应。 尤其是历史事件对当前事件的影响。</p><p>通过把成对的向量映射到基本速率和历史影响，从而把低维向量被输入Hawkes过程。 </p><p>另外历史邻居当前邻居的影响，不同节点是不同的，所以本文使用attention model来学习历史邻居对当前邻居影响的量化表示。</p><p>值得注意的是，本文目标是优化邻域生成序列的极大似然估计即<strong>条件强度函数</strong>（conditional intensity function）来邻域生成序列的到达率，而不是条件概率函数</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>本文通过跟踪节点邻域的形成来捕获网络的形成过程。<br><strong>Definition 1</strong> : 时序网络 $G=(V,E,A)$, $A$ 是事件集， 边$(x,y) \in E$ 被表示为按时间顺序的时间序列，例如， $\mathbf{a}_{x,y}=\{a_1\to{a_2}\to{…}\}\subset\mathcal{A}$, $a_i$ 表示时间$t_i$时刻的一个事件。  </p><p>因此，网络中节点的相邻邻居可以根据与邻居的交互事件的时序被组织为序列，表示邻域形成过程。</p><p><strong>Definition 2</strong> : 对于给定节点$x$,邻域表示为$N(x)=\{y_i|i=1,2…\}$.$x$的目标邻居到达事件可以表示为$\{x:(y_1,t_1)\to(y_2,t_2)\to…\to(y_n,t_n)\}$,即邻域形成序列。每个元组表示在时间戳$t_i$时，$x$与$y_i$建立边。</p><h3 id="Hawkes-Process"><a href="#Hawkes-Process" class="headerlink" title="Hawkes Process"></a>Hawkes Process</h3><p>点过程（Point Process）通过假设t时刻前的历史事件可以影响当前事件的发生，来对离散序列事件建模。<br>对于一个给定的节点$x \in V$, 在$x$的邻域生成序列中，到达目标邻居$y$的条件强度函数（或者可以说是$x$与$y$有边的可能性强度）可以表示为：  </p><script type="math/tex; mode=display">\tilde{\lambda}_{y|x}(t)=\mu_{x,y}+\sum_{t_h<t}{\alpha_{h,y}\kappa(t-t_{h})}</script><p>其中，$\mu_{x,y}$表示构建一条连接节点$x$和$y$的基本率(base rate)，$h$是t时刻前的历史目标节点，$\alpha_{h,y}$表示一个$t_h$时刻的历史目标节点$h$（该节点是$x$的邻居）对当前邻居$y$的影响强度。$\sum_{t_h&lt;t}$表示遍历t时刻前$x$的所有邻居。$\kappa(t-t_{h})$表示随时间的衰减，可以表示成指数函数：  </p><script type="math/tex; mode=display">\kappa(t-t_h)=\exp(-\delta_s(t-t_h))</script><p>其中，减少率 $\delta$是一个源依赖参数，对于每一个源节点（每个序列的根），历史邻居对当前邻居形成的影响强度是不同的。具体来说，如果$\kappa$越大，说明$t_h$时刻的邻居对当前邻居的影响越大，即 $-\delta_s(t-t_h)$越大, $\delta_s(t-t_h)$越小，因为$t$是当前时刻的邻居，所以当$t_h$越接近当前邻居时刻时，$\kappa$越大，这就说明了里当前时刻之前越近的邻居，对当前时刻邻居的影响越大。<br>综上所述，$\kappa$的具体意义是随时间衰减的影响，其中$\delta_s$参数表示对于不同的源节点，影响是不同的。</p><p>如果$\tilde{\lambda}_{y|x}(t)$ 越大，说明x和y有边的可能性也越大。</p><p>直观的来看，基本率（base rate）$\mu_{x,y}$揭示了节点x和节点y之间的连接可能性。为了简洁，本文使用了<strong>负平方欧式距离（negative squared Euclidean）</strong>来反映表示向量间的相似度: $\mu_{x,y}=f(\mathbf{e}_x,\mathbf{e}_y)=-||\mathbf{e}_x-\mathbf{e}_y||^2$。同样的，在计算历史邻居$h$对当前邻居$y$的影响时，也采用这个方法，即： $\alpha_{h,y}=f(\mathbf{e}_h,\mathbf{e}_y)=-||\mathbf{e}_h-\mathbf{e}_y||^2$。<br>因为条件强度函数必须为正，所以使用如下公式: $\lambda_{y|x}(t)=\exp(\tilde\lambda_{y|x}(t))$。$exp()$对原函数进行了归一化，所以问题就转化为了given $x$, maximize likelihood: $p(y|x)$. 这就与传统的NE方法差不多了。。。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>根据论文中（3）式，可以看出，$\sum_{t_h&lt;t}{\alpha_{h,y}\kappa(t-t_{h})}$这一部分主要描述了历史邻居对当前邻居的影响，但是完全忽略了源节点$x$，因为源节点$x$的变化也会影响到历史邻居对当前邻居的亲近程度(affinity)。因此，本文引入了<strong>attention model</strong>。as follows：  </p><script type="math/tex; mode=display">w_{h,x} = \frac{\exp(-||\mathbf{e}_x-\mathbf{e}_h||^2)}{\sum_{h'}{\exp(-||\mathbf{e}_x-\mathbf{e}_{h'}||^2)}}</script><p>这是一个softmax函数 来根据源节点$x$的不同为它的邻居赋予不同权重。</p><p>最后， 历史邻居与当前邻居的连接紧密程度可以表示为:</p><script type="math/tex; mode=display">\alpha_{h,y}=w_{h,x}f(\mathbf{e}_h,\mathbf{e}_y)</script><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>目标函数即为给定节点$x$以及基于邻域形成序列的霍克斯过程, 生成节点$y$的条件概率。 公式如下：</p><script type="math/tex; mode=display">p(y|x, \mathcal{H}_x(t)) = \frac{\lambda_{y|x}(t)}{\sum_{y'}{\lambda_{y'|x}(t)}}</script><p>目标函数即为所有节点对的极大似然：</p><script type="math/tex; mode=display">\log \mathcal{L}=\sum_{x\in{\mathcal{V}}}{\sum_{y\in{\mathcal{H}_x}}}{\log{p(y|x,\mathcal{H}(t))}}</script><p>最后，由于softmax过程是calculating expensive，所以采用负采样优化损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3220054&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HTNE&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Int
      
    
    </summary>
    
      <category term="Network Embedding" scheme="http://yoursite.com/categories/Network-Embedding/"/>
    
      <category term="paper" scheme="http://yoursite.com/categories/Network-Embedding/paper/"/>
    
    
      <category term="Hawkes process" scheme="http://yoursite.com/tags/Hawkes-process/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/01/16/hello-world/"/>
    <id>http://yoursite.com/2019/01/16/hello-world/</id>
    <published>2019-01-15T16:00:00.000Z</published>
    <updated>2019-01-17T06:48:31.411Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="test" scheme="http://yoursite.com/categories/test/"/>
    
    
  </entry>
  
</feed>
